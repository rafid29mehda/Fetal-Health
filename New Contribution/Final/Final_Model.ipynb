{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DM5rY681dftk",
        "outputId": "f69a1728-070a-4029-d019-4afbf58600c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pytorch-tabnet in /usr/local/lib/python3.11/dist-packages (4.1.0)\n",
            "Requirement already satisfied: ctgan in /usr/local/lib/python3.11/dist-packages (0.11.0)\n",
            "Requirement already satisfied: optuna in /usr/local/lib/python3.11/dist-packages (4.2.1)\n",
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.11/dist-packages (0.13.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: scipy>1.4 in /usr/local/lib/python3.11/dist-packages (from pytorch-tabnet) (1.13.1)\n",
            "Requirement already satisfied: tqdm>=4.36 in /usr/local/lib/python3.11/dist-packages (from pytorch-tabnet) (4.67.1)\n",
            "Requirement already satisfied: rdt>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from ctgan) (1.14.0)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (1.15.1)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.11/dist-packages (from optuna) (6.9.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (24.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.38)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n",
            "Requirement already satisfied: sklearn-compat<1,>=0.1 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (0.1.3)\n",
            "Requirement already satisfied: joblib<2,>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl<4,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (3.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.1)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (1.3.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: Faker>=17 in /usr/local/lib/python3.11/dist-packages (from rdt>=1.14.0->ctgan) (36.2.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.1.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "# Install libraries\n",
        "!pip install pytorch-tabnet ctgan optuna imbalanced-learn torch pandas numpy scikit-learn matplotlib seaborn\n",
        "\n",
        "# Imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from pytorch_tabnet.tab_model import TabNetClassifier\n",
        "import optuna\n",
        "from optuna import Trial\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from ctgan import CTGAN\n",
        "import pickle\n",
        "\n",
        "# Set seeds\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess\n",
        "data = pd.read_csv('/content/fetal_health.csv')\n",
        "features_to_drop = [\n",
        "    'fetal_movement', 'histogram_width', 'histogram_max', 'mean_value_of_long_term_variability',\n",
        "    'histogram_number_of_peaks', 'light_decelerations', 'histogram_tendency',\n",
        "    'histogram_number_of_zeroes', 'severe_decelerations', 'baseline value', 'histogram_min'\n",
        "]\n",
        "data_dropped = data.drop(columns=features_to_drop)\n",
        "data_dropped['fetal_health'] = data_dropped['fetal_health'].astype(int)\n",
        "X = data_dropped.drop(['fetal_health'], axis=1)\n",
        "y = data_dropped['fetal_health']\n",
        "\n",
        "# Scale and simulate temporal data\n",
        "scaler = MinMaxScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "X_scaled = pd.DataFrame(X_scaled, columns=X.columns)\n",
        "n_time_steps = 5\n",
        "X_temporal = []\n",
        "y_temporal = []\n",
        "for i in range(len(X_scaled)):\n",
        "    sample = X_scaled.iloc[i].values\n",
        "    time_series = []\n",
        "    for t in range(n_time_steps):\n",
        "        noise = np.random.uniform(-0.05, 0.05, size=sample.shape)\n",
        "        noisy_sample = np.clip(sample + noise, 0, 1)\n",
        "        time_series.append(noisy_sample)\n",
        "    X_temporal.append(time_series)\n",
        "    y_temporal.append(y.iloc[i])\n",
        "X_temporal = np.array(X_temporal)\n",
        "y_temporal = np.array(y_temporal)"
      ],
      "metadata": {
        "id": "pOzrlYsCd1Fs"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Temporal CTGAN\n",
        "minority_mask = np.isin(y_temporal, [2, 3])\n",
        "X_minority_temporal = X_temporal[minority_mask]\n",
        "y_minority_temporal = y_temporal[minority_mask]\n",
        "X_minority_flat = X_minority_temporal.reshape(len(X_minority_temporal), -1)\n",
        "feature_names = [f'f{t}_{col}' for t in range(n_time_steps) for col in X.columns]\n",
        "data_minority_flat = pd.DataFrame(X_minority_flat, columns=feature_names)\n",
        "data_minority_flat['fetal_health'] = y_minority_temporal\n",
        "n_samples_adjusted = (len(data_minority_flat) // 10) * 10\n",
        "data_minority_trimmed = data_minority_flat.iloc[:n_samples_adjusted]\n",
        "suspect_data = data_minority_trimmed[data_minority_trimmed['fetal_health'] == 2]\n",
        "pathological_data = data_minority_trimmed[data_minority_trimmed['fetal_health'] == 3]\n",
        "ctgan_suspect = CTGAN(epochs=500, batch_size=50, verbose=True, cuda=True)\n",
        "ctgan_suspect.fit(suspect_data, discrete_columns=['fetal_health'])\n",
        "ctgan_pathological = CTGAN(epochs=500, batch_size=50, verbose=True, cuda=True)\n",
        "ctgan_pathological.fit(pathological_data, discrete_columns=['fetal_health'])\n",
        "n_suspect = 1655 - 295\n",
        "n_pathological = 1655 - 176\n",
        "synthetic_suspect = ctgan_suspect.sample(n_suspect)\n",
        "synthetic_pathological = ctgan_pathological.sample(n_pathological)\n",
        "synthetic_data = pd.concat([synthetic_suspect, synthetic_pathological], ignore_index=True)\n",
        "synthetic_flat = synthetic_data.drop('fetal_health', axis=1).values\n",
        "synthetic_labels = synthetic_data['fetal_health'].values\n",
        "X_synthetic_temporal = synthetic_flat.reshape(-1, n_time_steps, X_scaled.shape[1])\n",
        "X_gan_temporal = np.vstack([X_temporal, X_synthetic_temporal])\n",
        "y_gan_temporal = np.hstack([y_temporal, synthetic_labels])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "onyv-e1yd1H6",
        "outputId": "37aa73bb-06f3-45f2-b68e-11d72f72f9f3"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Gen. (-7.67) | Discrim. (2.85): 100%|██████████| 500/500 [03:02<00:00,  2.74it/s]\n",
            "Gen. (-3.05) | Discrim. (0.66): 100%|██████████| 500/500 [01:43<00:00,  4.83it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split and flatten\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_gan_temporal, y_gan_temporal, test_size=0.3, random_state=42, stratify=y_gan_temporal\n",
        ")\n",
        "X_train_final, X_valid, y_train_final, y_valid = train_test_split(\n",
        "    X_train, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
        ")\n",
        "X_train_flat = X_train_final.reshape(-1, n_time_steps * X_scaled.shape[1])\n",
        "X_valid_flat = X_valid.reshape(-1, n_time_steps * X_scaled.shape[1])\n",
        "X_test_flat = X_test.reshape(-1, n_time_steps * X_scaled.shape[1])"
      ],
      "metadata": {
        "id": "JLRzv6lrd1Mq"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Optuna tuning (keep your original function)\n",
        "def objective(trial: Trial):\n",
        "    n_d = trial.suggest_int('n_d', 32, 128)\n",
        "    n_a = trial.suggest_int('n_a', 32, 128)\n",
        "    n_steps = trial.suggest_int('n_steps', 3, 10)\n",
        "    gamma = trial.suggest_float('gamma', 1.0, 2.0)\n",
        "    lambda_sparse = trial.suggest_float('lambda_sparse', 1e-4, 1e-2, log=True)\n",
        "    learning_rate = trial.suggest_float('learning_rate', 1e-3, 1e-1, log=True)\n",
        "    batch_size = trial.suggest_categorical('batch_size', [128, 256, 512])\n",
        "    tabnet = UncertaintyTabNet(\n",
        "        input_dim=n_time_steps * X_scaled.shape[1],\n",
        "        output_dim=3,\n",
        "        n_d=n_d, n_a=n_a, n_steps=n_steps, gamma=gamma,\n",
        "        lambda_sparse=lambda_sparse, optimizer_fn=torch.optim.Adam,\n",
        "        optimizer_params=dict(lr=learning_rate), mask_type='sparsemax', verbose=0\n",
        "    )\n",
        "    tabnet.fit(X_train_flat, y_train_final, eval_set=[(X_valid_flat, y_valid)], max_epochs=100, patience=20, batch_size=batch_size)\n",
        "    y_pred = np.argmax(tabnet.predict_proba(X_valid_flat)[0], axis=1) + 1\n",
        "    return accuracy_score(y_valid, y_pred)\n",
        "\n",
        "study = optuna.create_study(direction='maximize')\n",
        "study.optimize(objective, n_trials=50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O5IdrZeid1O4",
        "outputId": "bbf1af93-c400-4cb2-8982-3aaa9a98fd97"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-03-07 02:55:22,784] A new study created in memory with name: no-name-97dea92b-dc8d-4038-9312-42f4908fd035\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 61 with best_epoch = 41 and best_val_0_accuracy = 0.96115\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "<ipython-input-7-0eb623f98781>:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  x = torch.tensor(x, dtype=torch.float32).to(self.device)\n",
            "[I 2025-03-07 02:55:44,117] Trial 0 finished with value: 0.9194244604316547 and parameters: {'n_d': 124, 'n_a': 65, 'n_steps': 3, 'gamma': 1.9630249468244363, 'lambda_sparse': 0.0005042765167811175, 'learning_rate': 0.03010336640258786, 'batch_size': 256}. Best is trial 0 with value: 0.9194244604316547.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 81 with best_epoch = 61 and best_val_0_accuracy = 0.95683\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "<ipython-input-7-0eb623f98781>:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  x = torch.tensor(x, dtype=torch.float32).to(self.device)\n",
            "[I 2025-03-07 02:57:50,038] Trial 1 finished with value: 0.9223021582733812 and parameters: {'n_d': 34, 'n_a': 63, 'n_steps': 10, 'gamma': 1.3340807236336336, 'lambda_sparse': 0.0017945006834948739, 'learning_rate': 0.013709541533710467, 'batch_size': 128}. Best is trial 1 with value: 0.9223021582733812.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 79 with best_epoch = 59 and best_val_0_accuracy = 0.95827\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "<ipython-input-7-0eb623f98781>:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  x = torch.tensor(x, dtype=torch.float32).to(self.device)\n",
            "[I 2025-03-07 02:58:34,363] Trial 2 finished with value: 0.9194244604316547 and parameters: {'n_d': 90, 'n_a': 66, 'n_steps': 6, 'gamma': 1.5009343242709567, 'lambda_sparse': 0.004866672178797903, 'learning_rate': 0.0041001491118923825, 'batch_size': 256}. Best is trial 1 with value: 0.9223021582733812.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stop training because you reached max_epochs = 100 with best_epoch = 82 and best_val_0_accuracy = 0.93957\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "<ipython-input-7-0eb623f98781>:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  x = torch.tensor(x, dtype=torch.float32).to(self.device)\n",
            "[I 2025-03-07 02:59:25,291] Trial 3 finished with value: 0.8805755395683453 and parameters: {'n_d': 58, 'n_a': 115, 'n_steps': 9, 'gamma': 1.1453635547124041, 'lambda_sparse': 0.0003144356108325272, 'learning_rate': 0.0024939943818654627, 'batch_size': 512}. Best is trial 1 with value: 0.9223021582733812.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 81 with best_epoch = 61 and best_val_0_accuracy = 0.95827\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "<ipython-input-7-0eb623f98781>:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  x = torch.tensor(x, dtype=torch.float32).to(self.device)\n",
            "[I 2025-03-07 03:00:16,609] Trial 4 finished with value: 0.9007194244604316 and parameters: {'n_d': 71, 'n_a': 68, 'n_steps': 7, 'gamma': 1.568216331210424, 'lambda_sparse': 0.0004244325810457374, 'learning_rate': 0.01719760456821651, 'batch_size': 256}. Best is trial 1 with value: 0.9223021582733812.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stop training because you reached max_epochs = 100 with best_epoch = 95 and best_val_0_accuracy = 0.96403\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "<ipython-input-7-0eb623f98781>:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  x = torch.tensor(x, dtype=torch.float32).to(self.device)\n",
            "[I 2025-03-07 03:01:27,281] Trial 5 finished with value: 0.9035971223021583 and parameters: {'n_d': 94, 'n_a': 111, 'n_steps': 8, 'gamma': 1.0137454041777603, 'lambda_sparse': 0.00016296782276411124, 'learning_rate': 0.0067964854080733005, 'batch_size': 256}. Best is trial 1 with value: 0.9223021582733812.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 86 with best_epoch = 66 and best_val_0_accuracy = 0.94964\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "<ipython-input-7-0eb623f98781>:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  x = torch.tensor(x, dtype=torch.float32).to(self.device)\n",
            "[I 2025-03-07 03:02:36,379] Trial 6 finished with value: 0.9237410071942446 and parameters: {'n_d': 87, 'n_a': 72, 'n_steps': 9, 'gamma': 1.6944940725885331, 'lambda_sparse': 0.002578175420119197, 'learning_rate': 0.007582636874942687, 'batch_size': 256}. Best is trial 6 with value: 0.9237410071942446.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 67 with best_epoch = 47 and best_val_0_accuracy = 0.95827\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "<ipython-input-7-0eb623f98781>:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  x = torch.tensor(x, dtype=torch.float32).to(self.device)\n",
            "[I 2025-03-07 03:03:14,380] Trial 7 finished with value: 0.9294964028776979 and parameters: {'n_d': 108, 'n_a': 126, 'n_steps': 6, 'gamma': 1.2980492007975326, 'lambda_sparse': 0.009777585109528013, 'learning_rate': 0.00708404067797675, 'batch_size': 256}. Best is trial 7 with value: 0.9294964028776979.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stop training because you reached max_epochs = 100 with best_epoch = 87 and best_val_0_accuracy = 0.93094\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "<ipython-input-7-0eb623f98781>:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  x = torch.tensor(x, dtype=torch.float32).to(self.device)\n",
            "[I 2025-03-07 03:03:50,158] Trial 8 finished with value: 0.8820143884892087 and parameters: {'n_d': 63, 'n_a': 88, 'n_steps': 6, 'gamma': 1.426833280512413, 'lambda_sparse': 0.00010912910343871081, 'learning_rate': 0.002438161936521401, 'batch_size': 512}. Best is trial 7 with value: 0.9294964028776979.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 65 with best_epoch = 45 and best_val_0_accuracy = 0.96259\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "<ipython-input-7-0eb623f98781>:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  x = torch.tensor(x, dtype=torch.float32).to(self.device)\n",
            "[I 2025-03-07 03:04:32,260] Trial 9 finished with value: 0.9237410071942446 and parameters: {'n_d': 56, 'n_a': 107, 'n_steps': 7, 'gamma': 1.7094500814347806, 'lambda_sparse': 0.00017376086197339182, 'learning_rate': 0.08756631792557358, 'batch_size': 256}. Best is trial 7 with value: 0.9294964028776979.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 79 with best_epoch = 59 and best_val_0_accuracy = 0.96403\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "<ipython-input-7-0eb623f98781>:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  x = torch.tensor(x, dtype=torch.float32).to(self.device)\n",
            "[I 2025-03-07 03:05:30,465] Trial 10 finished with value: 0.9093525179856116 and parameters: {'n_d': 124, 'n_a': 34, 'n_steps': 4, 'gamma': 1.2548361729417232, 'lambda_sparse': 0.008940664052242869, 'learning_rate': 0.0014033827160205381, 'batch_size': 128}. Best is trial 7 with value: 0.9294964028776979.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 84 with best_epoch = 64 and best_val_0_accuracy = 0.96259\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "<ipython-input-7-0eb623f98781>:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  x = torch.tensor(x, dtype=torch.float32).to(self.device)\n",
            "[I 2025-03-07 03:06:11,360] Trial 11 finished with value: 0.9366906474820144 and parameters: {'n_d': 106, 'n_a': 94, 'n_steps': 5, 'gamma': 1.7973039867640046, 'lambda_sparse': 0.002030353387074685, 'learning_rate': 0.007161810317674455, 'batch_size': 256}. Best is trial 11 with value: 0.9366906474820144.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 41 with best_epoch = 21 and best_val_0_accuracy = 0.95252\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "<ipython-input-7-0eb623f98781>:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  x = torch.tensor(x, dtype=torch.float32).to(self.device)\n",
            "[I 2025-03-07 03:06:31,759] Trial 12 finished with value: 0.935251798561151 and parameters: {'n_d': 108, 'n_a': 128, 'n_steps': 5, 'gamma': 1.9867027363006597, 'lambda_sparse': 0.0011528108536512044, 'learning_rate': 0.02963482593627118, 'batch_size': 256}. Best is trial 11 with value: 0.9366906474820144.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 48 with best_epoch = 28 and best_val_0_accuracy = 0.96259\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "<ipython-input-7-0eb623f98781>:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  x = torch.tensor(x, dtype=torch.float32).to(self.device)\n",
            "[I 2025-03-07 03:07:07,538] Trial 13 finished with value: 0.9467625899280575 and parameters: {'n_d': 107, 'n_a': 91, 'n_steps': 4, 'gamma': 1.9862819354663797, 'lambda_sparse': 0.001075111739678473, 'learning_rate': 0.038645230585066505, 'batch_size': 128}. Best is trial 13 with value: 0.9467625899280575.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 48 with best_epoch = 28 and best_val_0_accuracy = 0.95683\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "<ipython-input-7-0eb623f98781>:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  x = torch.tensor(x, dtype=torch.float32).to(self.device)\n",
            "[I 2025-03-07 03:07:36,844] Trial 14 finished with value: 0.9482014388489208 and parameters: {'n_d': 105, 'n_a': 91, 'n_steps': 3, 'gamma': 1.8138986388704432, 'lambda_sparse': 0.0008616669322260091, 'learning_rate': 0.09341153714462574, 'batch_size': 128}. Best is trial 14 with value: 0.9482014388489208.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 84 with best_epoch = 64 and best_val_0_accuracy = 0.96691\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "<ipython-input-7-0eb623f98781>:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  x = torch.tensor(x, dtype=torch.float32).to(self.device)\n",
            "[I 2025-03-07 03:08:27,313] Trial 15 finished with value: 0.9453237410071943 and parameters: {'n_d': 102, 'n_a': 93, 'n_steps': 3, 'gamma': 1.8516741236058816, 'lambda_sparse': 0.0007533447218152833, 'learning_rate': 0.07697329653437242, 'batch_size': 128}. Best is trial 14 with value: 0.9482014388489208.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 44 with best_epoch = 24 and best_val_0_accuracy = 0.96403\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "<ipython-input-7-0eb623f98781>:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  x = torch.tensor(x, dtype=torch.float32).to(self.device)\n",
            "[I 2025-03-07 03:09:00,413] Trial 16 finished with value: 0.9410071942446043 and parameters: {'n_d': 79, 'n_a': 49, 'n_steps': 4, 'gamma': 1.8628515588763475, 'lambda_sparse': 0.0010569293582312245, 'learning_rate': 0.045069782036480384, 'batch_size': 128}. Best is trial 14 with value: 0.9482014388489208.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 43 with best_epoch = 23 and best_val_0_accuracy = 0.96835\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "<ipython-input-7-0eb623f98781>:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  x = torch.tensor(x, dtype=torch.float32).to(self.device)\n",
            "[I 2025-03-07 03:09:32,605] Trial 17 finished with value: 0.939568345323741 and parameters: {'n_d': 117, 'n_a': 82, 'n_steps': 4, 'gamma': 1.6666434969044166, 'lambda_sparse': 0.003698730941666416, 'learning_rate': 0.050755753554490475, 'batch_size': 128}. Best is trial 14 with value: 0.9482014388489208.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 50 with best_epoch = 30 and best_val_0_accuracy = 0.95396\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "<ipython-input-7-0eb623f98781>:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  x = torch.tensor(x, dtype=torch.float32).to(self.device)\n",
            "[I 2025-03-07 03:10:02,910] Trial 18 finished with value: 0.9482014388489208 and parameters: {'n_d': 94, 'n_a': 101, 'n_steps': 3, 'gamma': 1.9125404466285452, 'lambda_sparse': 0.0006826603550649701, 'learning_rate': 0.09622567235748893, 'batch_size': 128}. Best is trial 14 with value: 0.9482014388489208.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 39 with best_epoch = 19 and best_val_0_accuracy = 0.95827\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "<ipython-input-7-0eb623f98781>:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  x = torch.tensor(x, dtype=torch.float32).to(self.device)\n",
            "[I 2025-03-07 03:10:26,720] Trial 19 finished with value: 0.9496402877697842 and parameters: {'n_d': 80, 'n_a': 103, 'n_steps': 3, 'gamma': 1.7754600590221106, 'lambda_sparse': 0.0006762710167411346, 'learning_rate': 0.09740040556227401, 'batch_size': 128}. Best is trial 19 with value: 0.9496402877697842.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stop training because you reached max_epochs = 100 with best_epoch = 94 and best_val_0_accuracy = 0.96978\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "<ipython-input-7-0eb623f98781>:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  x = torch.tensor(x, dtype=torch.float32).to(self.device)\n",
            "[I 2025-03-07 03:10:57,104] Trial 20 finished with value: 0.9366906474820144 and parameters: {'n_d': 41, 'n_a': 117, 'n_steps': 5, 'gamma': 1.5960184809868543, 'lambda_sparse': 0.00025727963765251235, 'learning_rate': 0.02038540215131327, 'batch_size': 512}. Best is trial 19 with value: 0.9496402877697842.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 57 with best_epoch = 37 and best_val_0_accuracy = 0.96547\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "<ipython-input-7-0eb623f98781>:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  x = torch.tensor(x, dtype=torch.float32).to(self.device)\n",
            "[I 2025-03-07 03:11:32,068] Trial 21 finished with value: 0.9424460431654677 and parameters: {'n_d': 74, 'n_a': 104, 'n_steps': 3, 'gamma': 1.7918873206139336, 'lambda_sparse': 0.0006492375782753446, 'learning_rate': 0.09910573553256893, 'batch_size': 128}. Best is trial 19 with value: 0.9496402877697842.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 51 with best_epoch = 31 and best_val_0_accuracy = 0.96259\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "<ipython-input-7-0eb623f98781>:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  x = torch.tensor(x, dtype=torch.float32).to(self.device)\n",
            "[I 2025-03-07 03:12:02,943] Trial 22 finished with value: 0.9482014388489208 and parameters: {'n_d': 96, 'n_a': 96, 'n_steps': 3, 'gamma': 1.864595894994061, 'lambda_sparse': 0.0007144841056334167, 'learning_rate': 0.06611033406612978, 'batch_size': 128}. Best is trial 19 with value: 0.9496402877697842.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 46 with best_epoch = 26 and best_val_0_accuracy = 0.96835\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "<ipython-input-7-0eb623f98781>:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  x = torch.tensor(x, dtype=torch.float32).to(self.device)\n",
            "[I 2025-03-07 03:12:30,730] Trial 23 finished with value: 0.9366906474820144 and parameters: {'n_d': 84, 'n_a': 103, 'n_steps': 3, 'gamma': 1.7722346971150233, 'lambda_sparse': 0.0015795157043735429, 'learning_rate': 0.06049415317917969, 'batch_size': 128}. Best is trial 19 with value: 0.9496402877697842.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 41 with best_epoch = 21 and best_val_0_accuracy = 0.95971\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "<ipython-input-7-0eb623f98781>:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  x = torch.tensor(x, dtype=torch.float32).to(self.device)\n",
            "[I 2025-03-07 03:13:02,501] Trial 24 finished with value: 0.9280575539568345 and parameters: {'n_d': 97, 'n_a': 80, 'n_steps': 4, 'gamma': 1.9122799165984983, 'lambda_sparse': 0.0003816428073926918, 'learning_rate': 0.03198700015603835, 'batch_size': 128}. Best is trial 19 with value: 0.9496402877697842.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 66 with best_epoch = 46 and best_val_0_accuracy = 0.96403\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "<ipython-input-7-0eb623f98781>:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  x = torch.tensor(x, dtype=torch.float32).to(self.device)\n",
            "[I 2025-03-07 03:13:41,791] Trial 25 finished with value: 0.9410071942446043 and parameters: {'n_d': 115, 'n_a': 100, 'n_steps': 3, 'gamma': 1.7178499350107472, 'lambda_sparse': 0.0006338813217619956, 'learning_rate': 0.0924840851988296, 'batch_size': 128}. Best is trial 19 with value: 0.9496402877697842.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 86 with best_epoch = 66 and best_val_0_accuracy = 0.96547\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "<ipython-input-7-0eb623f98781>:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  x = torch.tensor(x, dtype=torch.float32).to(self.device)\n",
            "[I 2025-03-07 03:14:57,836] Trial 26 finished with value: 0.939568345323741 and parameters: {'n_d': 78, 'n_a': 83, 'n_steps': 5, 'gamma': 1.579413121409532, 'lambda_sparse': 0.00024584563083297165, 'learning_rate': 0.05458518310573015, 'batch_size': 128}. Best is trial 19 with value: 0.9496402877697842.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 57 with best_epoch = 37 and best_val_0_accuracy = 0.96547\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "<ipython-input-7-0eb623f98781>:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  x = torch.tensor(x, dtype=torch.float32).to(self.device)\n",
            "[I 2025-03-07 03:15:40,927] Trial 27 finished with value: 0.943884892086331 and parameters: {'n_d': 68, 'n_a': 119, 'n_steps': 4, 'gamma': 1.8888943459463756, 'lambda_sparse': 0.0013970854114794648, 'learning_rate': 0.021385532410360054, 'batch_size': 128}. Best is trial 19 with value: 0.9496402877697842.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stop training because you reached max_epochs = 100 with best_epoch = 88 and best_val_0_accuracy = 0.96259\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "<ipython-input-7-0eb623f98781>:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  x = torch.tensor(x, dtype=torch.float32).to(self.device)\n",
            "[I 2025-03-07 03:16:01,443] Trial 28 finished with value: 0.9424460431654677 and parameters: {'n_d': 85, 'n_a': 75, 'n_steps': 3, 'gamma': 1.791463159540924, 'lambda_sparse': 0.000517795511565343, 'learning_rate': 0.0676505819135273, 'batch_size': 512}. Best is trial 19 with value: 0.9496402877697842.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 54 with best_epoch = 34 and best_val_0_accuracy = 0.96403\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "<ipython-input-7-0eb623f98781>:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  x = torch.tensor(x, dtype=torch.float32).to(self.device)\n",
            "[I 2025-03-07 03:16:34,187] Trial 29 finished with value: 0.9381294964028777 and parameters: {'n_d': 126, 'n_a': 58, 'n_steps': 3, 'gamma': 1.6591612055778722, 'lambda_sparse': 0.0009041450025732648, 'learning_rate': 0.02961645233930599, 'batch_size': 128}. Best is trial 19 with value: 0.9496402877697842.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 45 with best_epoch = 25 and best_val_0_accuracy = 0.96403\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "<ipython-input-7-0eb623f98781>:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  x = torch.tensor(x, dtype=torch.float32).to(self.device)\n",
            "[I 2025-03-07 03:17:15,131] Trial 30 finished with value: 0.9338129496402877 and parameters: {'n_d': 101, 'n_a': 111, 'n_steps': 5, 'gamma': 1.9277841394180741, 'lambda_sparse': 0.00047305907518084035, 'learning_rate': 0.039953287116600664, 'batch_size': 128}. Best is trial 19 with value: 0.9496402877697842.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 51 with best_epoch = 31 and best_val_0_accuracy = 0.96835\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "<ipython-input-7-0eb623f98781>:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  x = torch.tensor(x, dtype=torch.float32).to(self.device)\n",
            "[I 2025-03-07 03:17:46,846] Trial 31 finished with value: 0.9381294964028777 and parameters: {'n_d': 92, 'n_a': 98, 'n_steps': 3, 'gamma': 1.8437447306692927, 'lambda_sparse': 0.0007796897713502987, 'learning_rate': 0.06864566353248276, 'batch_size': 128}. Best is trial 19 with value: 0.9496402877697842.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 72 with best_epoch = 52 and best_val_0_accuracy = 0.9554\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "<ipython-input-7-0eb623f98781>:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  x = torch.tensor(x, dtype=torch.float32).to(self.device)\n",
            "[I 2025-03-07 03:18:40,142] Trial 32 finished with value: 0.9309352517985612 and parameters: {'n_d': 98, 'n_a': 87, 'n_steps': 4, 'gamma': 1.9304438355088847, 'lambda_sparse': 0.0005567259895269818, 'learning_rate': 0.07664375590197794, 'batch_size': 128}. Best is trial 19 with value: 0.9496402877697842.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 35 with best_epoch = 15 and best_val_0_accuracy = 0.95396\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "<ipython-input-7-0eb623f98781>:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  x = torch.tensor(x, dtype=torch.float32).to(self.device)\n",
            "[I 2025-03-07 03:19:01,833] Trial 33 finished with value: 0.9424460431654677 and parameters: {'n_d': 116, 'n_a': 98, 'n_steps': 3, 'gamma': 1.7581713219369053, 'lambda_sparse': 0.0008208804589228011, 'learning_rate': 0.09814937457238325, 'batch_size': 128}. Best is trial 19 with value: 0.9496402877697842.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 73 with best_epoch = 53 and best_val_0_accuracy = 0.96547\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "<ipython-input-7-0eb623f98781>:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  x = torch.tensor(x, dtype=torch.float32).to(self.device)\n",
            "[I 2025-03-07 03:19:45,789] Trial 34 finished with value: 0.9424460431654677 and parameters: {'n_d': 93, 'n_a': 96, 'n_steps': 3, 'gamma': 1.8649562428272706, 'lambda_sparse': 0.002002201861093394, 'learning_rate': 0.055356344866363844, 'batch_size': 128}. Best is trial 19 with value: 0.9496402877697842.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 57 with best_epoch = 37 and best_val_0_accuracy = 0.96403\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "<ipython-input-7-0eb623f98781>:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  x = torch.tensor(x, dtype=torch.float32).to(self.device)\n",
            "[I 2025-03-07 03:21:20,368] Trial 35 finished with value: 0.9453237410071943 and parameters: {'n_d': 88, 'n_a': 107, 'n_steps': 10, 'gamma': 1.9969403508000727, 'lambda_sparse': 0.0003713830501678633, 'learning_rate': 0.013258456224676517, 'batch_size': 128}. Best is trial 19 with value: 0.9496402877697842.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stop training because you reached max_epochs = 100 with best_epoch = 95 and best_val_0_accuracy = 0.96259\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "<ipython-input-7-0eb623f98781>:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  x = torch.tensor(x, dtype=torch.float32).to(self.device)\n",
            "[I 2025-03-07 03:21:46,636] Trial 36 finished with value: 0.9424460431654677 and parameters: {'n_d': 81, 'n_a': 111, 'n_steps': 4, 'gamma': 1.5040092580795539, 'lambda_sparse': 0.0012422088100647907, 'learning_rate': 0.06578456384097821, 'batch_size': 512}. Best is trial 19 with value: 0.9496402877697842.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 61 with best_epoch = 41 and best_val_0_accuracy = 0.95971\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "<ipython-input-7-0eb623f98781>:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  x = torch.tensor(x, dtype=torch.float32).to(self.device)\n",
            "[I 2025-03-07 03:23:08,059] Trial 37 finished with value: 0.9323741007194245 and parameters: {'n_d': 112, 'n_a': 122, 'n_steps': 8, 'gamma': 1.8111269156969307, 'lambda_sparse': 0.0028458799949591905, 'learning_rate': 0.03852231130594281, 'batch_size': 128}. Best is trial 19 with value: 0.9496402877697842.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 87 with best_epoch = 67 and best_val_0_accuracy = 0.96978\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "<ipython-input-7-0eb623f98781>:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  x = torch.tensor(x, dtype=torch.float32).to(self.device)\n",
            "[I 2025-03-07 03:23:59,699] Trial 38 finished with value: 0.920863309352518 and parameters: {'n_d': 49, 'n_a': 75, 'n_steps': 3, 'gamma': 1.424840378727949, 'lambda_sparse': 0.00031284036442492437, 'learning_rate': 0.004741755881836303, 'batch_size': 128}. Best is trial 19 with value: 0.9496402877697842.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 64 with best_epoch = 44 and best_val_0_accuracy = 0.96835\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "<ipython-input-7-0eb623f98781>:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  x = torch.tensor(x, dtype=torch.float32).to(self.device)\n",
            "[I 2025-03-07 03:24:47,033] Trial 39 finished with value: 0.9223021582733812 and parameters: {'n_d': 100, 'n_a': 88, 'n_steps': 4, 'gamma': 1.7446030502514458, 'lambda_sparse': 0.0006726159347336368, 'learning_rate': 0.010692174635868445, 'batch_size': 128}. Best is trial 19 with value: 0.9496402877697842.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 95 with best_epoch = 75 and best_val_0_accuracy = 0.96115\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "<ipython-input-7-0eb623f98781>:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  x = torch.tensor(x, dtype=torch.float32).to(self.device)\n",
            "[I 2025-03-07 03:25:21,480] Trial 40 finished with value: 0.9410071942446043 and parameters: {'n_d': 75, 'n_a': 102, 'n_steps': 6, 'gamma': 1.6306799406719226, 'lambda_sparse': 0.00158145263343359, 'learning_rate': 0.07808096997022873, 'batch_size': 512}. Best is trial 19 with value: 0.9496402877697842.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 35 with best_epoch = 15 and best_val_0_accuracy = 0.95683\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "<ipython-input-7-0eb623f98781>:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  x = torch.tensor(x, dtype=torch.float32).to(self.device)\n",
            "[I 2025-03-07 03:25:42,644] Trial 41 finished with value: 0.9323741007194245 and parameters: {'n_d': 107, 'n_a': 91, 'n_steps': 3, 'gamma': 1.9538970680936867, 'lambda_sparse': 0.0009840080910412097, 'learning_rate': 0.048373306293195834, 'batch_size': 128}. Best is trial 19 with value: 0.9496402877697842.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 57 with best_epoch = 37 and best_val_0_accuracy = 0.95683\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "<ipython-input-7-0eb623f98781>:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  x = torch.tensor(x, dtype=torch.float32).to(self.device)\n",
            "[I 2025-03-07 03:26:25,269] Trial 42 finished with value: 0.9496402877697842 and parameters: {'n_d': 95, 'n_a': 90, 'n_steps': 4, 'gamma': 1.9472937088578544, 'lambda_sparse': 0.0010069482985211105, 'learning_rate': 0.0379532181924021, 'batch_size': 128}. Best is trial 19 with value: 0.9496402877697842.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 57 with best_epoch = 37 and best_val_0_accuracy = 0.96403\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "<ipython-input-7-0eb623f98781>:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  x = torch.tensor(x, dtype=torch.float32).to(self.device)\n",
            "[I 2025-03-07 03:27:00,116] Trial 43 finished with value: 0.9568345323741008 and parameters: {'n_d': 90, 'n_a': 86, 'n_steps': 3, 'gamma': 1.9080694902662798, 'lambda_sparse': 0.0005477009039361701, 'learning_rate': 0.09931532317631023, 'batch_size': 128}. Best is trial 43 with value: 0.9568345323741008.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 49 with best_epoch = 29 and best_val_0_accuracy = 0.95683\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "<ipython-input-7-0eb623f98781>:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  x = torch.tensor(x, dtype=torch.float32).to(self.device)\n",
            "[I 2025-03-07 03:27:37,024] Trial 44 finished with value: 0.9309352517985612 and parameters: {'n_d': 89, 'n_a': 85, 'n_steps': 4, 'gamma': 1.9347238590796434, 'lambda_sparse': 0.00047616556348112593, 'learning_rate': 0.09921483477612107, 'batch_size': 128}. Best is trial 43 with value: 0.9568345323741008.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 43 with best_epoch = 23 and best_val_0_accuracy = 0.95971\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "<ipython-input-7-0eb623f98781>:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  x = torch.tensor(x, dtype=torch.float32).to(self.device)\n",
            "[I 2025-03-07 03:28:28,537] Trial 45 finished with value: 0.9453237410071943 and parameters: {'n_d': 66, 'n_a': 79, 'n_steps': 7, 'gamma': 1.8260391346367566, 'lambda_sparse': 0.0005775307145482362, 'learning_rate': 0.08222297723625399, 'batch_size': 128}. Best is trial 43 with value: 0.9568345323741008.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 84 with best_epoch = 64 and best_val_0_accuracy = 0.96547\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "<ipython-input-7-0eb623f98781>:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  x = torch.tensor(x, dtype=torch.float32).to(self.device)\n",
            "[I 2025-03-07 03:28:55,847] Trial 46 finished with value: 0.9280575539568345 and parameters: {'n_d': 84, 'n_a': 67, 'n_steps': 3, 'gamma': 1.0570234112781778, 'lambda_sparse': 0.00031043460807718984, 'learning_rate': 0.025842869597522355, 'batch_size': 256}. Best is trial 43 with value: 0.9568345323741008.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 41 with best_epoch = 21 and best_val_0_accuracy = 0.95827\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "<ipython-input-7-0eb623f98781>:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  x = torch.tensor(x, dtype=torch.float32).to(self.device)\n",
            "[I 2025-03-07 03:29:33,235] Trial 47 finished with value: 0.935251798561151 and parameters: {'n_d': 103, 'n_a': 112, 'n_steps': 5, 'gamma': 1.8974662442248775, 'lambda_sparse': 0.000922112878217201, 'learning_rate': 0.04571178006854396, 'batch_size': 128}. Best is trial 43 with value: 0.9568345323741008.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 81 with best_epoch = 61 and best_val_0_accuracy = 0.95971\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "<ipython-input-7-0eb623f98781>:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  x = torch.tensor(x, dtype=torch.float32).to(self.device)\n",
            "[I 2025-03-07 03:31:30,837] Trial 48 finished with value: 0.9424460431654677 and parameters: {'n_d': 121, 'n_a': 106, 'n_steps': 9, 'gamma': 1.9719731125333773, 'lambda_sparse': 0.001285098987230846, 'learning_rate': 0.08029628578529649, 'batch_size': 128}. Best is trial 43 with value: 0.9568345323741008.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 82 with best_epoch = 62 and best_val_0_accuracy = 0.95827\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "<ipython-input-7-0eb623f98781>:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  x = torch.tensor(x, dtype=torch.float32).to(self.device)\n",
            "[I 2025-03-07 03:32:04,146] Trial 49 finished with value: 0.9035971223021583 and parameters: {'n_d': 94, 'n_a': 61, 'n_steps': 4, 'gamma': 1.7149074499650465, 'lambda_sparse': 0.00019380709621412556, 'learning_rate': 0.0013717896244188701, 'batch_size': 256}. Best is trial 43 with value: 0.9568345323741008.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from pytorch_tabnet.tab_model import TabNetClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Assuming device, study.best_params, X_train_flat, y_train_final, X_valid_flat, y_valid, X_test_flat, y_test are defined earlier\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Define the augment_data function (from your original ModelAttentionMask.py)\n",
        "def augment_data(X, y, permutation_prob=0.1):\n",
        "    \"\"\"\n",
        "    Augment the dataset by randomly permuting feature orders with a given probability.\n",
        "\n",
        "    Parameters:\n",
        "    - X (numpy.ndarray): Feature matrix (e.g., shape (samples, 50) for 5 time steps × 10 features).\n",
        "    - y (numpy.ndarray): Target vector.\n",
        "    - permutation_prob (float): Probability of permuting each sample.\n",
        "\n",
        "    Returns:\n",
        "    - X_augmented (numpy.ndarray): Augmented feature matrix.\n",
        "    - y_augmented (numpy.ndarray): Augmented target vector.\n",
        "    \"\"\"\n",
        "    X_augmented = []\n",
        "    y_augmented = []\n",
        "    for sample, label in zip(X, y):\n",
        "        if np.random.rand() < permutation_prob:\n",
        "            perm = np.random.permutation(sample.shape[0])  # Permute the 50 features\n",
        "            sample = sample[perm]\n",
        "        X_augmented.append(sample)\n",
        "        y_augmented.append(label)\n",
        "    return np.array(X_augmented), np.array(y_augmented)\n",
        "\n",
        "# Define UncertaintyTabNet\n",
        "class UncertaintyTabNet(TabNetClassifier):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.dropout = nn.Dropout(p=0.3).to(device)\n",
        "        self.training = True\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.tensor(x, dtype=torch.float32).to(self.device)\n",
        "        if self.training or self.dropout.training:\n",
        "            x = self.dropout(x)\n",
        "        return self.network(x)\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        self.network.eval()\n",
        "        with torch.no_grad():\n",
        "            X_tensor = torch.tensor(X, dtype=torch.float32).to(self.device)\n",
        "            probs = []\n",
        "            for _ in range(50):\n",
        "                self.network.train()\n",
        "                logits, _ = self.forward(X_tensor)\n",
        "                prob = torch.softmax(logits, dim=1).cpu().numpy()\n",
        "                probs.append(prob)\n",
        "            probs = np.stack(probs, axis=0)\n",
        "        return np.mean(probs, axis=0), np.std(probs, axis=0)\n",
        "\n",
        "# Train with best params (assuming study.best_params is from your Optuna tuning)\n",
        "perm_reg_tabnet = UncertaintyTabNet(\n",
        "    input_dim=n_time_steps * X_scaled.shape[1],  # e.g., 50 (5 time steps × 10 features)\n",
        "    output_dim=3,\n",
        "    n_d=study.best_params['n_d'],\n",
        "    n_a=study.best_params['n_a'],\n",
        "    n_steps=study.best_params['n_steps'],\n",
        "    gamma=study.best_params['gamma'],\n",
        "    lambda_sparse=study.best_params['lambda_sparse'],\n",
        "    optimizer_fn=torch.optim.Adam,\n",
        "    optimizer_params={'lr': study.best_params['learning_rate']},\n",
        "    mask_type='sparsemax',\n",
        "    verbose=1,\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "# Apply data augmentation\n",
        "X_train_augmented, y_train_augmented = augment_data(X_train_flat, y_train_final, permutation_prob=0.1)\n",
        "\n",
        "# Train the model\n",
        "perm_reg_tabnet.fit(\n",
        "    X_train=X_train_augmented,\n",
        "    y_train=y_train_augmented,\n",
        "    eval_set=[(X_valid_flat, y_valid)],\n",
        "    eval_name=['valid'],\n",
        "    eval_metric=['accuracy'],\n",
        "    max_epochs=100,\n",
        "    patience=20,\n",
        "    batch_size=study.best_params['batch_size'],\n",
        "    virtual_batch_size=128\n",
        ")\n",
        "\n",
        "# Evaluate\n",
        "probs_mean, probs_std = perm_reg_tabnet.predict_proba(X_test_flat)\n",
        "y_pred_mean = np.argmax(probs_mean, axis=1) + 1  # Adjust back to 1, 2, 3\n",
        "y_pred_uncertainty = np.max(probs_std, axis=1)\n",
        "\n",
        "print(\"\\nTemporal Uncertainty-Aware TabNet Classification Report:\")\n",
        "print(classification_report(y_test, y_pred_mean, target_names=['Normal', 'Suspect', 'Pathological']))\n",
        "print(f\"Mean uncertainty: {np.mean(y_pred_uncertainty):.4f}\")\n",
        "\n",
        "# Confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(confusion_matrix(y_test, y_pred_mean), annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=['Normal', 'Suspect', 'Pathological'],\n",
        "            yticklabels=['Normal', 'Suspect', 'Pathological'])\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.show()\n",
        "\n",
        "# Uncertainty distribution\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.histplot(y_pred_uncertainty, bins=20, color='purple')\n",
        "plt.title('Prediction Uncertainty Distribution')\n",
        "plt.xlabel('Max Standard Deviation')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "pIRGdoFhd1RP",
        "outputId": "79cf298f-6dd2-4e1d-f10a-99507ac45a1d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0  | loss: 1.29399 | valid_accuracy: 0.66475 |  0:00:03s\n",
            "epoch 1  | loss: 0.47326 | valid_accuracy: 0.60288 |  0:00:04s\n",
            "epoch 2  | loss: 0.36569 | valid_accuracy: 0.87626 |  0:00:05s\n",
            "epoch 3  | loss: 0.38026 | valid_accuracy: 0.82878 |  0:00:06s\n",
            "epoch 4  | loss: 0.36027 | valid_accuracy: 0.82734 |  0:00:07s\n",
            "epoch 5  | loss: 0.36852 | valid_accuracy: 0.92806 |  0:00:07s\n",
            "epoch 6  | loss: 0.33633 | valid_accuracy: 0.93094 |  0:00:08s\n",
            "epoch 7  | loss: 0.34548 | valid_accuracy: 0.88777 |  0:00:08s\n",
            "epoch 8  | loss: 0.34305 | valid_accuracy: 0.87338 |  0:00:09s\n",
            "epoch 9  | loss: 0.30961 | valid_accuracy: 0.91942 |  0:00:09s\n",
            "epoch 10 | loss: 0.30153 | valid_accuracy: 0.91511 |  0:00:10s\n",
            "epoch 11 | loss: 0.32439 | valid_accuracy: 0.93525 |  0:00:10s\n",
            "epoch 12 | loss: 0.31323 | valid_accuracy: 0.9295  |  0:00:11s\n",
            "epoch 13 | loss: 0.28025 | valid_accuracy: 0.92518 |  0:00:11s\n",
            "epoch 14 | loss: 0.29375 | valid_accuracy: 0.92086 |  0:00:12s\n",
            "epoch 15 | loss: 0.2721  | valid_accuracy: 0.94532 |  0:00:13s\n",
            "epoch 16 | loss: 0.27281 | valid_accuracy: 0.94388 |  0:00:14s\n",
            "epoch 17 | loss: 0.30999 | valid_accuracy: 0.95108 |  0:00:14s\n",
            "epoch 18 | loss: 0.26961 | valid_accuracy: 0.94101 |  0:00:15s\n",
            "epoch 19 | loss: 0.26979 | valid_accuracy: 0.93381 |  0:00:15s\n",
            "epoch 20 | loss: 0.2622  | valid_accuracy: 0.94245 |  0:00:16s\n",
            "epoch 21 | loss: 0.25566 | valid_accuracy: 0.94964 |  0:00:16s\n",
            "epoch 22 | loss: 0.24754 | valid_accuracy: 0.95396 |  0:00:17s\n",
            "epoch 23 | loss: 0.2628  | valid_accuracy: 0.94388 |  0:00:18s\n",
            "epoch 24 | loss: 0.28004 | valid_accuracy: 0.93381 |  0:00:18s\n",
            "epoch 25 | loss: 0.25936 | valid_accuracy: 0.94676 |  0:00:19s\n",
            "epoch 26 | loss: 0.27318 | valid_accuracy: 0.93381 |  0:00:19s\n",
            "epoch 27 | loss: 0.24817 | valid_accuracy: 0.95108 |  0:00:20s\n",
            "epoch 28 | loss: 0.25527 | valid_accuracy: 0.96115 |  0:00:20s\n",
            "epoch 29 | loss: 0.23351 | valid_accuracy: 0.9482  |  0:00:21s\n",
            "epoch 30 | loss: 0.31342 | valid_accuracy: 0.92806 |  0:00:21s\n",
            "epoch 31 | loss: 0.25643 | valid_accuracy: 0.95396 |  0:00:22s\n",
            "epoch 32 | loss: 0.25232 | valid_accuracy: 0.95971 |  0:00:23s\n",
            "epoch 33 | loss: 0.25672 | valid_accuracy: 0.95252 |  0:00:23s\n",
            "epoch 34 | loss: 0.23902 | valid_accuracy: 0.9554  |  0:00:24s\n",
            "epoch 35 | loss: 0.23102 | valid_accuracy: 0.94676 |  0:00:24s\n",
            "epoch 36 | loss: 0.24631 | valid_accuracy: 0.94964 |  0:00:25s\n",
            "epoch 37 | loss: 0.25684 | valid_accuracy: 0.9482  |  0:00:26s\n",
            "epoch 38 | loss: 0.25361 | valid_accuracy: 0.96115 |  0:00:26s\n",
            "epoch 39 | loss: 0.25783 | valid_accuracy: 0.94964 |  0:00:27s\n",
            "epoch 40 | loss: 0.25786 | valid_accuracy: 0.94532 |  0:00:27s\n",
            "epoch 41 | loss: 0.23947 | valid_accuracy: 0.95683 |  0:00:28s\n",
            "epoch 42 | loss: 0.23885 | valid_accuracy: 0.95108 |  0:00:29s\n",
            "epoch 43 | loss: 0.24746 | valid_accuracy: 0.94676 |  0:00:29s\n",
            "epoch 44 | loss: 0.22372 | valid_accuracy: 0.94964 |  0:00:30s\n",
            "epoch 45 | loss: 0.21817 | valid_accuracy: 0.94676 |  0:00:30s\n",
            "epoch 46 | loss: 0.22979 | valid_accuracy: 0.91655 |  0:00:31s\n",
            "epoch 47 | loss: 0.25566 | valid_accuracy: 0.95827 |  0:00:32s\n",
            "epoch 48 | loss: 0.23599 | valid_accuracy: 0.95396 |  0:00:32s\n",
            "\n",
            "Early stopping occurred at epoch 48 with best_epoch = 28 and best_valid_accuracy = 0.96115\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "<ipython-input-14-3f657da0463c>:45: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  x = torch.tensor(x, dtype=torch.float32).to(self.device)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Temporal Uncertainty-Aware TabNet Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      Normal       0.95      0.93      0.94       496\n",
            "     Suspect       0.91      0.97      0.94       497\n",
            "Pathological       1.00      0.96      0.98       497\n",
            "\n",
            "    accuracy                           0.95      1490\n",
            "   macro avg       0.95      0.95      0.95      1490\n",
            "weighted avg       0.95      0.95      0.95      1490\n",
            "\n",
            "Mean uncertainty: 0.2533\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yiRXzbpSd1TV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qn0Q5ESzd1Vb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xdYH4Od3d1Xh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DuqhKwhbd1Z-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WLgrjqNEd1cF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "K1unnG4Zdw92"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "N0_hFHzZdxAF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sCLQJQx7dxCl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "X6BpKSnfdxEw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JGkZ1jezdxHO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SQZgp3aTdxJS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}