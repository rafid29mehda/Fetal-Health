{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NFjtCN7ACc0q",
        "outputId": "8cf6c269-1ed3-405e-a2df-8eacd7c09094"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pytorch-tabnet in /usr/local/lib/python3.11/dist-packages (4.1.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from pytorch-tabnet) (1.26.4)\n",
            "Requirement already satisfied: scikit_learn>0.21 in /usr/local/lib/python3.11/dist-packages (from pytorch-tabnet) (1.6.1)\n",
            "Requirement already satisfied: scipy>1.4 in /usr/local/lib/python3.11/dist-packages (from pytorch-tabnet) (1.13.1)\n",
            "Requirement already satisfied: torch>=1.3 in /usr/local/lib/python3.11/dist-packages (from pytorch-tabnet) (2.5.1+cu124)\n",
            "Requirement already satisfied: tqdm>=4.36 in /usr/local/lib/python3.11/dist-packages (from pytorch-tabnet) (4.67.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit_learn>0.21->pytorch-tabnet) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit_learn>0.21->pytorch-tabnet) (3.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.3->pytorch-tabnet) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3->pytorch-tabnet) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.3->pytorch-tabnet) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3->pytorch-tabnet) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.3->pytorch-tabnet) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3->pytorch-tabnet) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3->pytorch-tabnet) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3->pytorch-tabnet) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3->pytorch-tabnet) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3->pytorch-tabnet) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3->pytorch-tabnet) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3->pytorch-tabnet) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3->pytorch-tabnet) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3->pytorch-tabnet) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3->pytorch-tabnet) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3->pytorch-tabnet) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3->pytorch-tabnet) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3->pytorch-tabnet) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3->pytorch-tabnet) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.3->pytorch-tabnet) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.3->pytorch-tabnet) (3.0.2)\n",
            "Requirement already satisfied: captum in /usr/local/lib/python3.11/dist-packages (0.7.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from captum) (3.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from captum) (1.26.4)\n",
            "Requirement already satisfied: torch>=1.6 in /usr/local/lib/python3.11/dist-packages (from captum) (2.5.1+cu124)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from captum) (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.6->captum) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6->captum) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.6->captum) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6->captum) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.6->captum) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6->captum) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6->captum) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6->captum) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6->captum) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6->captum) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6->captum) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6->captum) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6->captum) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6->captum) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6->captum) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6->captum) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6->captum) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6->captum) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6->captum) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.6->captum) (1.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->captum) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->captum) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->captum) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->captum) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->captum) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->captum) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->captum) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->captum) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->captum) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.6->captum) (3.0.2)\n",
            "Requirement already satisfied: optuna in /usr/local/lib/python3.11/dist-packages (4.2.1)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (1.14.1)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.11/dist-packages (from optuna) (6.9.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (24.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.38)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (1.3.9)\n",
            "Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.12.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.1.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.11/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.2)\n",
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.11/dist-packages (0.13.0)\n",
            "Requirement already satisfied: numpy<3,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy<2,>=1.10.1 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn<2,>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (1.6.1)\n",
            "Requirement already satisfied: sklearn-compat<1,>=0.1 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (0.1.3)\n",
            "Requirement already satisfied: joblib<2,>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl<4,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (3.5.0)\n",
            "Requirement already satisfied: dask-expr in /usr/local/lib/python3.11/dist-packages (1.1.19)\n",
            "Requirement already satisfied: dask==2024.11.2 in /usr/local/lib/python3.11/dist-packages (from dask-expr) (2024.11.2)\n",
            "Requirement already satisfied: pyarrow>=14.0.1 in /usr/local/lib/python3.11/dist-packages (from dask-expr) (18.1.0)\n",
            "Requirement already satisfied: pandas>=2 in /usr/local/lib/python3.11/dist-packages (from dask-expr) (2.2.2)\n",
            "Requirement already satisfied: click>=8.1 in /usr/local/lib/python3.11/dist-packages (from dask==2024.11.2->dask-expr) (8.1.8)\n",
            "Requirement already satisfied: cloudpickle>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from dask==2024.11.2->dask-expr) (3.1.1)\n",
            "Requirement already satisfied: fsspec>=2021.09.0 in /usr/local/lib/python3.11/dist-packages (from dask==2024.11.2->dask-expr) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from dask==2024.11.2->dask-expr) (24.2)\n",
            "Requirement already satisfied: partd>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from dask==2024.11.2->dask-expr) (1.4.2)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from dask==2024.11.2->dask-expr) (6.0.2)\n",
            "Requirement already satisfied: toolz>=0.10.0 in /usr/local/lib/python3.11/dist-packages (from dask==2024.11.2->dask-expr) (0.12.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.13.0 in /usr/local/lib/python3.11/dist-packages (from dask==2024.11.2->dask-expr) (8.6.1)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=2->dask-expr) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=2->dask-expr) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=2->dask-expr) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=2->dask-expr) (2025.1)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata>=4.13.0->dask==2024.11.2->dask-expr) (3.21.0)\n",
            "Requirement already satisfied: locket in /usr/local/lib/python3.11/dist-packages (from partd>=1.4.0->dask==2024.11.2->dask-expr) (1.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=2->dask-expr) (1.17.0)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement scikit-learn-contrib (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for scikit-learn-contrib\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: lightgbm in /usr/local/lib/python3.11/dist-packages (4.5.0)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from lightgbm) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from lightgbm) (1.13.1)\n",
            "Requirement already satisfied: ctgan in /usr/local/lib/python3.11/dist-packages (0.11.0)\n",
            "Requirement already satisfied: numpy>=1.23.3 in /usr/local/lib/python3.11/dist-packages (from ctgan) (1.26.4)\n",
            "Requirement already satisfied: pandas>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from ctgan) (2.2.2)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ctgan) (2.5.1+cu124)\n",
            "Requirement already satisfied: tqdm<5,>=4.29 in /usr/local/lib/python3.11/dist-packages (from ctgan) (4.67.1)\n",
            "Requirement already satisfied: rdt>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from ctgan) (1.14.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.5.0->ctgan) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.5.0->ctgan) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.5.0->ctgan) (2025.1)\n",
            "Requirement already satisfied: scipy>=1.9.2 in /usr/local/lib/python3.11/dist-packages (from rdt>=1.14.0->ctgan) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from rdt>=1.14.0->ctgan) (1.6.1)\n",
            "Requirement already satisfied: Faker>=17 in /usr/local/lib/python3.11/dist-packages (from rdt>=1.14.0->ctgan) (36.1.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->ctgan) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->ctgan) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->ctgan) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->ctgan) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->ctgan) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->ctgan) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->ctgan) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->ctgan) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->ctgan) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->ctgan) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->ctgan) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->ctgan) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->ctgan) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->ctgan) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->ctgan) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->ctgan) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->ctgan) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->ctgan) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->ctgan) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->ctgan) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.5.0->ctgan) (1.17.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.1.3->rdt>=1.14.0->ctgan) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.1.3->rdt>=1.14.0->ctgan) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->ctgan) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "# Install necessary libraries\n",
        "!pip install pytorch-tabnet\n",
        "!pip install captum\n",
        "!pip install optuna\n",
        "!pip install imbalanced-learn\n",
        "!pip install dask-expr\n",
        "!pip install scikit-learn-contrib\n",
        "!pip install lightgbm\n",
        "!pip install ctgan  # Add this for CTGAN\n",
        "\n",
        "# Data manipulation and analysis\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Preprocessing and modeling\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "\n",
        "# Deep Learning Model\n",
        "from pytorch_tabnet.tab_model import TabNetClassifier\n",
        "\n",
        "# Explainable AI\n",
        "import shap\n",
        "\n",
        "# Hyperparameter Optimization\n",
        "import optuna\n",
        "from optuna import Trial\n",
        "\n",
        "# Suppress warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# For model saving and loading\n",
        "import joblib\n",
        "import pickle  # Add pickle for CTGAN loading\n",
        "\n",
        "# Import torch for TabNet\n",
        "import torch\n",
        "\n",
        "# Additional imports for Permutation Regularization\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------\n",
        "# 3. Load and Preprocess the Dataset\n",
        "# -------------------\n",
        "# Load the dataset\n",
        "data = pd.read_csv('/content/fetal_health.csv')  # Update path if needed\n",
        "\n",
        "# Display the first five rows to verify\n",
        "print(\"First five rows of the dataset:\")\n",
        "print(data.head())\n",
        "\n",
        "# Check the shape of the dataset\n",
        "print(f\"\\nDataset Shape: {data.shape}\")\n",
        "\n",
        "# Features to drop based on prior analysis (same as original)\n",
        "features_to_drop = [\n",
        "    'fetal_movement', 'histogram_width', 'histogram_max', 'mean_value_of_long_term_variability',\n",
        "    'histogram_number_of_peaks', 'light_decelerations', 'histogram_tendency',\n",
        "    'histogram_number_of_zeroes', 'severe_decelerations', 'baseline value', 'histogram_min'\n",
        "]\n",
        "\n",
        "# Drop the specified features\n",
        "data_dropped = data.drop(columns=features_to_drop)\n",
        "\n",
        "# Verify the remaining features\n",
        "print(\"\\nFeatures after dropping less important ones:\")\n",
        "print(data_dropped.columns.tolist())\n",
        "\n",
        "# Check the new shape of the dataset\n",
        "print(f\"\\nNew Dataset Shape after dropping features: {data_dropped.shape}\")\n",
        "\n",
        "# Convert 'fetal_health' to integer\n",
        "data_dropped['fetal_health'] = data_dropped['fetal_health'].astype(int)\n",
        "\n",
        "# Mapping numerical classes to descriptive labels\n",
        "health_mapping = {1: 'Normal', 2: 'Suspect', 3: 'Pathological'}\n",
        "data_dropped['fetal_health_label'] = data_dropped['fetal_health'].map(health_mapping)\n",
        "\n",
        "# Display the mapping\n",
        "print(\"\\nDataset with Mapped Labels:\")\n",
        "print(data_dropped[['fetal_health', 'fetal_health_label']].head())\n",
        "\n",
        "# Features and target\n",
        "X = data_dropped.drop(['fetal_health', 'fetal_health_label'], axis=1)\n",
        "y = data_dropped['fetal_health']\n",
        "\n",
        "# --- Replace ADASYN + Tomek with CTGAN ---\n",
        "# Load pre-trained CTGAN models (upload ctgan_suspect.pkl and ctgan_pathological.pkl to Colab first)\n",
        "with open('/content/ctgan_suspect.pkl', 'rb') as f:\n",
        "    ctgan_suspect = pickle.load(f)\n",
        "with open('/content/ctgan_pathological.pkl', 'rb') as f:\n",
        "    ctgan_pathological = pickle.load(f)\n",
        "\n",
        "# Scale original features (CTGAN models were trained on scaled data)\n",
        "scaler = MinMaxScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "X_scaled = pd.DataFrame(X_scaled, columns=X.columns)\n",
        "\n",
        "# Number of synthetic samples to balance with Normal (1655)\n",
        "n_suspect = 1655 - 295  # 1360\n",
        "n_pathological = 1655 - 176  # 1479\n",
        "\n",
        "# Generate synthetic samples using CTGAN\n",
        "synthetic_suspect = ctgan_suspect.sample(n_suspect)\n",
        "synthetic_pathological = ctgan_pathological.sample(n_pathological)\n",
        "\n",
        "# Combine synthetic data\n",
        "synthetic_data = pd.concat([synthetic_suspect, synthetic_pathological], ignore_index=True)\n",
        "synthetic_df = synthetic_data.drop('fetal_health', axis=1)\n",
        "synthetic_labels = synthetic_data['fetal_health']\n",
        "\n",
        "# Combine with original data\n",
        "X_resampled = pd.concat([X_scaled, synthetic_df], ignore_index=True)\n",
        "y_resampled = pd.concat([y, synthetic_labels], ignore_index=True)\n",
        "\n",
        "# Display the shape and class distribution\n",
        "print(f\"\\nResampled X shape after CTGAN: {X_resampled.shape}\")\n",
        "print(f\"Resampled y distribution after CTGAN:\\n{y_resampled.value_counts()}\")\n",
        "\n",
        "# Visualize the resampled class distribution\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.countplot(x=y_resampled, palette='viridis')\n",
        "plt.title('Class Distribution After CTGAN')\n",
        "plt.xlabel('Fetal Health')\n",
        "plt.ylabel('Count')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "PJm0K2WBC44r",
        "outputId": "99959150-2615-41f8-8fd3-4f8b1c1f1f73"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First five rows of the dataset:\n",
            "   baseline value  accelerations  fetal_movement  uterine_contractions  \\\n",
            "0           120.0          0.000             0.0                 0.000   \n",
            "1           132.0          0.006             0.0                 0.006   \n",
            "2           133.0          0.003             0.0                 0.008   \n",
            "3           134.0          0.003             0.0                 0.008   \n",
            "4           132.0          0.007             0.0                 0.008   \n",
            "\n",
            "   light_decelerations  severe_decelerations  prolongued_decelerations  \\\n",
            "0                0.000                   0.0                       0.0   \n",
            "1                0.003                   0.0                       0.0   \n",
            "2                0.003                   0.0                       0.0   \n",
            "3                0.003                   0.0                       0.0   \n",
            "4                0.000                   0.0                       0.0   \n",
            "\n",
            "   abnormal_short_term_variability  mean_value_of_short_term_variability  \\\n",
            "0                             73.0                                   0.5   \n",
            "1                             17.0                                   2.1   \n",
            "2                             16.0                                   2.1   \n",
            "3                             16.0                                   2.4   \n",
            "4                             16.0                                   2.4   \n",
            "\n",
            "   percentage_of_time_with_abnormal_long_term_variability  ...  histogram_min  \\\n",
            "0                                               43.0       ...           62.0   \n",
            "1                                                0.0       ...           68.0   \n",
            "2                                                0.0       ...           68.0   \n",
            "3                                                0.0       ...           53.0   \n",
            "4                                                0.0       ...           53.0   \n",
            "\n",
            "   histogram_max  histogram_number_of_peaks  histogram_number_of_zeroes  \\\n",
            "0          126.0                        2.0                         0.0   \n",
            "1          198.0                        6.0                         1.0   \n",
            "2          198.0                        5.0                         1.0   \n",
            "3          170.0                       11.0                         0.0   \n",
            "4          170.0                        9.0                         0.0   \n",
            "\n",
            "   histogram_mode  histogram_mean  histogram_median  histogram_variance  \\\n",
            "0           120.0           137.0             121.0                73.0   \n",
            "1           141.0           136.0             140.0                12.0   \n",
            "2           141.0           135.0             138.0                13.0   \n",
            "3           137.0           134.0             137.0                13.0   \n",
            "4           137.0           136.0             138.0                11.0   \n",
            "\n",
            "   histogram_tendency  fetal_health  \n",
            "0                 1.0           2.0  \n",
            "1                 0.0           1.0  \n",
            "2                 0.0           1.0  \n",
            "3                 1.0           1.0  \n",
            "4                 1.0           1.0  \n",
            "\n",
            "[5 rows x 22 columns]\n",
            "\n",
            "Dataset Shape: (2126, 22)\n",
            "\n",
            "Features after dropping less important ones:\n",
            "['accelerations', 'uterine_contractions', 'prolongued_decelerations', 'abnormal_short_term_variability', 'mean_value_of_short_term_variability', 'percentage_of_time_with_abnormal_long_term_variability', 'histogram_mode', 'histogram_mean', 'histogram_median', 'histogram_variance', 'fetal_health']\n",
            "\n",
            "New Dataset Shape after dropping features: (2126, 11)\n",
            "\n",
            "Dataset with Mapped Labels:\n",
            "   fetal_health fetal_health_label\n",
            "0             2            Suspect\n",
            "1             1             Normal\n",
            "2             1             Normal\n",
            "3             1             Normal\n",
            "4             1             Normal\n",
            "\n",
            "Resampled X shape after CTGAN: (4965, 10)\n",
            "Resampled y distribution after CTGAN:\n",
            "fetal_health\n",
            "2    1655\n",
            "1    1655\n",
            "3    1655\n",
            "Name: count, dtype: int64\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the resampled data (70% train, 30% test) with stratification\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_resampled, y_resampled, test_size=0.3, random_state=42, stratify=y_resampled\n",
        ")\n",
        "\n",
        "# Display the shapes of the training and testing sets\n",
        "print(f\"\\nTraining set shape: {X_train.shape}\")\n",
        "print(f\"Testing set shape: {X_test.shape}\")\n",
        "\n",
        "# Initialize the MinMaxScaler (already scaled, but reassign for consistency)\n",
        "X_train_scaled = pd.DataFrame(X_train, columns=X.columns)\n",
        "X_test_scaled = pd.DataFrame(X_test, columns=X.columns)\n",
        "\n",
        "# Verify scaling by checking min and max values\n",
        "print(\"\\nMin of Scaled Training Features (Should be 0):\")\n",
        "print(X_train_scaled.min())\n",
        "print(\"\\nMax of Scaled Training Features (Should be 1):\")\n",
        "print(X_train_scaled.max())\n",
        "\n",
        "# Adjust the target values so they start from 0\n",
        "y_train = y_train - 1\n",
        "y_test = y_test - 1\n",
        "\n",
        "# Display the adjusted target distributions\n",
        "print(\"\\nAdjusted y_train distribution:\")\n",
        "print(pd.Series(y_train).value_counts())\n",
        "print(\"\\nAdjusted y_test distribution:\")\n",
        "print(pd.Series(y_test).value_counts())\n",
        "\n",
        "# Further split the training data into training and validation sets (80% train, 20% validation)\n",
        "X_train_final, X_valid, y_train_final, y_valid = train_test_split(\n",
        "    X_train_scaled, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
        ")\n",
        "\n",
        "# Display the shapes of the final training and validation sets\n",
        "print(f\"\\nFinal Training set shape: {X_train_final.shape}\")\n",
        "print(f\"Validation set shape: {X_valid.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LGbS2Q4hC46q",
        "outputId": "db3c9a16-d21a-4ba1-c5b4-6b1d46a2fd3b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training set shape: (3475, 10)\n",
            "Testing set shape: (1490, 10)\n",
            "\n",
            "Min of Scaled Training Features (Should be 0):\n",
            "accelerations                                            -0.114280\n",
            "uterine_contractions                                     -0.337867\n",
            "prolongued_decelerations                                 -0.252885\n",
            "abnormal_short_term_variability                          -0.123320\n",
            "mean_value_of_short_term_variability                     -0.146603\n",
            "percentage_of_time_with_abnormal_long_term_variability   -0.166119\n",
            "histogram_mode                                           -0.268054\n",
            "histogram_mean                                           -0.122386\n",
            "histogram_median                                         -0.156802\n",
            "histogram_variance                                       -0.299106\n",
            "dtype: float64\n",
            "\n",
            "Max of Scaled Training Features (Should be 1):\n",
            "accelerations                                             0.947368\n",
            "uterine_contractions                                      1.183233\n",
            "prolongued_decelerations                                  1.179175\n",
            "abnormal_short_term_variability                           1.132907\n",
            "mean_value_of_short_term_variability                      1.540372\n",
            "percentage_of_time_with_abnormal_long_term_variability    1.373118\n",
            "histogram_mode                                            1.000000\n",
            "histogram_mean                                            1.000000\n",
            "histogram_median                                          1.000000\n",
            "histogram_variance                                        1.506365\n",
            "dtype: float64\n",
            "\n",
            "Adjusted y_train distribution:\n",
            "fetal_health\n",
            "0    1159\n",
            "1    1158\n",
            "2    1158\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Adjusted y_test distribution:\n",
            "fetal_health\n",
            "1    497\n",
            "2    497\n",
            "0    496\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Final Training set shape: (2780, 10)\n",
            "Validation set shape: (695, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the objective function for Optuna\n",
        "def objective(trial: Trial):\n",
        "    # Define the hyperparameter space\n",
        "    n_d = trial.suggest_int('n_d', 32, 128)\n",
        "    n_a = trial.suggest_int('n_a', 32, 128)\n",
        "    n_steps = trial.suggest_int('n_steps', 3, 10)\n",
        "    gamma = trial.suggest_float('gamma', 1.0, 2.0)\n",
        "    lambda_sparse = trial.suggest_float('lambda_sparse', 1e-4, 1e-2, log=True)\n",
        "    learning_rate = trial.suggest_float('learning_rate', 1e-3, 1e-1, log=True)\n",
        "    batch_size = trial.suggest_categorical('batch_size', [128, 256, 512])\n",
        "\n",
        "    # Initialize TabNet with current hyperparameters\n",
        "    tabnet = TabNetClassifier(\n",
        "        n_d=n_d,\n",
        "        n_a=n_a,\n",
        "        n_steps=n_steps,\n",
        "        gamma=gamma,\n",
        "        lambda_sparse=lambda_sparse,\n",
        "        optimizer_fn=torch.optim.Adam,\n",
        "        optimizer_params=dict(lr=learning_rate),\n",
        "        mask_type='sparsemax',\n",
        "        verbose=0\n",
        "    )\n",
        "\n",
        "    # Train the model on the final training set\n",
        "    tabnet.fit(\n",
        "        X_train=X_train_final.values,\n",
        "        y_train=y_train_final.values,\n",
        "        eval_set=[(X_valid.values, y_valid.values)],\n",
        "        eval_name=['valid'],\n",
        "        eval_metric=['accuracy'],\n",
        "        max_epochs=100,\n",
        "        patience=20,\n",
        "        batch_size=batch_size,\n",
        "        virtual_batch_size=128,\n",
        "        num_workers=0,\n",
        "        drop_last=False\n",
        "    )\n",
        "\n",
        "    # Predict on the validation set\n",
        "    y_pred = tabnet.predict(X_valid.values)\n",
        "    accuracy = accuracy_score(y_valid, y_pred)\n",
        "\n",
        "    return accuracy\n",
        "\n",
        "# Create and optimize the Optuna study\n",
        "study = optuna.create_study(direction='maximize', study_name='TabNet Hyperparameter Optimization')\n",
        "study.optimize(objective, n_trials=50, timeout=3600)  # Adjust n_trials and timeout as needed\n",
        "\n",
        "# Display the best hyperparameters and validation accuracy\n",
        "print(\"Best Hyperparameters: \", study.best_params)\n",
        "print(\"Best Validation Accuracy: \", study.best_value)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W5WLPGMeMFpY",
        "outputId": "b63009ca-dd93-4f63-cee5-5f06802fb76b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-03-01 05:32:07,545] A new study created in memory with name: TabNet Hyperparameter Optimization\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stop training because you reached max_epochs = 100 with best_epoch = 96 and best_valid_accuracy = 0.94676\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-03-01 05:33:28,564] Trial 0 finished with value: 0.9467625899280575 and parameters: {'n_d': 69, 'n_a': 108, 'n_steps': 7, 'gamma': 1.8130168201748105, 'lambda_sparse': 0.000593536406492887, 'learning_rate': 0.02007527513864254, 'batch_size': 256}. Best is trial 0 with value: 0.9467625899280575.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 85 with best_epoch = 65 and best_valid_accuracy = 0.9223\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-03-01 05:34:10,249] Trial 1 finished with value: 0.9223021582733812 and parameters: {'n_d': 59, 'n_a': 52, 'n_steps': 8, 'gamma': 1.6585383753433987, 'lambda_sparse': 0.0002047708040076128, 'learning_rate': 0.0256740883732765, 'batch_size': 512}. Best is trial 0 with value: 0.9467625899280575.\n",
            "[I 2025-03-01 05:34:38,830] Trial 2 finished with value: 0.9251798561151079 and parameters: {'n_d': 39, 'n_a': 53, 'n_steps': 4, 'gamma': 1.5218765611554943, 'lambda_sparse': 0.00010847642191020549, 'learning_rate': 0.0033485269614736447, 'batch_size': 512}. Best is trial 0 with value: 0.9467625899280575.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stop training because you reached max_epochs = 100 with best_epoch = 97 and best_valid_accuracy = 0.92518\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-03-01 05:35:11,988] Trial 3 finished with value: 0.9280575539568345 and parameters: {'n_d': 70, 'n_a': 69, 'n_steps': 3, 'gamma': 1.4208144754779668, 'lambda_sparse': 0.006040621736588851, 'learning_rate': 0.001178566855644744, 'batch_size': 256}. Best is trial 0 with value: 0.9467625899280575.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 84 with best_epoch = 64 and best_valid_accuracy = 0.92806\n",
            "Stop training because you reached max_epochs = 100 with best_epoch = 96 and best_valid_accuracy = 0.91799\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-03-01 05:36:05,645] Trial 4 finished with value: 0.9179856115107914 and parameters: {'n_d': 115, 'n_a': 82, 'n_steps': 8, 'gamma': 1.537978032139152, 'lambda_sparse': 0.00032895542567258515, 'learning_rate': 0.0025860895795922485, 'batch_size': 512}. Best is trial 0 with value: 0.9467625899280575.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stop training because you reached max_epochs = 100 with best_epoch = 91 and best_valid_accuracy = 0.90216\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-03-01 05:36:48,530] Trial 5 finished with value: 0.902158273381295 and parameters: {'n_d': 119, 'n_a': 74, 'n_steps': 7, 'gamma': 1.6714321376989836, 'lambda_sparse': 0.001095232971957352, 'learning_rate': 0.0010061437834230566, 'batch_size': 512}. Best is trial 0 with value: 0.9467625899280575.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stop training because you reached max_epochs = 100 with best_epoch = 99 and best_valid_accuracy = 0.92662\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-03-01 05:37:31,988] Trial 6 finished with value: 0.9266187050359712 and parameters: {'n_d': 115, 'n_a': 91, 'n_steps': 7, 'gamma': 1.2636874282156965, 'lambda_sparse': 0.0057957737186864, 'learning_rate': 0.010382098278671694, 'batch_size': 512}. Best is trial 0 with value: 0.9467625899280575.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 80 with best_epoch = 60 and best_valid_accuracy = 0.93669\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-03-01 05:38:30,962] Trial 7 finished with value: 0.9366906474820144 and parameters: {'n_d': 33, 'n_a': 83, 'n_steps': 8, 'gamma': 1.030945696534022, 'lambda_sparse': 0.00011255479592218362, 'learning_rate': 0.0019276632725064044, 'batch_size': 256}. Best is trial 0 with value: 0.9467625899280575.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 86 with best_epoch = 66 and best_valid_accuracy = 0.94388\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-03-01 05:39:14,127] Trial 8 finished with value: 0.943884892086331 and parameters: {'n_d': 92, 'n_a': 66, 'n_steps': 5, 'gamma': 1.7928900152681244, 'lambda_sparse': 0.00268009477839061, 'learning_rate': 0.026631735451122854, 'batch_size': 256}. Best is trial 0 with value: 0.9467625899280575.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 97 with best_epoch = 77 and best_valid_accuracy = 0.91942\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-03-01 05:39:56,690] Trial 9 finished with value: 0.9194244604316547 and parameters: {'n_d': 44, 'n_a': 71, 'n_steps': 7, 'gamma': 1.975073896411791, 'lambda_sparse': 0.0005323455316685954, 'learning_rate': 0.01284212909731312, 'batch_size': 512}. Best is trial 0 with value: 0.9467625899280575.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 81 with best_epoch = 61 and best_valid_accuracy = 0.93381\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-03-01 05:42:05,356] Trial 10 finished with value: 0.9338129496402877 and parameters: {'n_d': 93, 'n_a': 121, 'n_steps': 10, 'gamma': 1.9849671831300837, 'lambda_sparse': 0.0011899379823264013, 'learning_rate': 0.09016342096667075, 'batch_size': 128}. Best is trial 0 with value: 0.9467625899280575.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 64 with best_epoch = 44 and best_valid_accuracy = 0.92374\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-03-01 05:42:38,433] Trial 11 finished with value: 0.9237410071942446 and parameters: {'n_d': 90, 'n_a': 106, 'n_steps': 5, 'gamma': 1.7948788815304966, 'lambda_sparse': 0.0026487784200583185, 'learning_rate': 0.033610993069671524, 'batch_size': 256}. Best is trial 0 with value: 0.9467625899280575.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stop training because you reached max_epochs = 100 with best_epoch = 82 and best_valid_accuracy = 0.9482\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-03-01 05:43:28,304] Trial 12 finished with value: 0.9482014388489208 and parameters: {'n_d': 82, 'n_a': 39, 'n_steps': 5, 'gamma': 1.8199525370756227, 'lambda_sparse': 0.002110417717319748, 'learning_rate': 0.029058894037876853, 'batch_size': 256}. Best is trial 12 with value: 0.9482014388489208.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 27 with best_epoch = 7 and best_valid_accuracy = 0.84317\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-03-01 05:43:42,389] Trial 13 finished with value: 0.8431654676258993 and parameters: {'n_d': 72, 'n_a': 34, 'n_steps': 5, 'gamma': 1.7984650260751982, 'lambda_sparse': 0.0006110981776147322, 'learning_rate': 0.07031276877618949, 'batch_size': 256}. Best is trial 12 with value: 0.9482014388489208.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 86 with best_epoch = 66 and best_valid_accuracy = 0.93957\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-03-01 05:45:10,918] Trial 14 finished with value: 0.939568345323741 and parameters: {'n_d': 58, 'n_a': 123, 'n_steps': 6, 'gamma': 1.8635970604718013, 'lambda_sparse': 0.002204397590901215, 'learning_rate': 0.007502098559629983, 'batch_size': 128}. Best is trial 12 with value: 0.9482014388489208.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 82 with best_epoch = 62 and best_valid_accuracy = 0.91511\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-03-01 05:46:24,278] Trial 15 finished with value: 0.9151079136690647 and parameters: {'n_d': 83, 'n_a': 104, 'n_steps': 10, 'gamma': 1.6575075832741788, 'lambda_sparse': 0.009858398776915163, 'learning_rate': 0.03993680651956455, 'batch_size': 256}. Best is trial 12 with value: 0.9482014388489208.\n",
            "[I 2025-03-01 05:46:54,199] Trial 16 finished with value: 0.9467625899280575 and parameters: {'n_d': 103, 'n_a': 37, 'n_steps': 3, 'gamma': 1.3267658077689395, 'lambda_sparse': 0.0014853710465568084, 'learning_rate': 0.016498931144576595, 'batch_size': 256}. Best is trial 12 with value: 0.9482014388489208.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 87 with best_epoch = 67 and best_valid_accuracy = 0.94676\n",
            "Stop training because you reached max_epochs = 100 with best_epoch = 93 and best_valid_accuracy = 0.9223\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-03-01 05:47:51,811] Trial 17 finished with value: 0.9223021582733812 and parameters: {'n_d': 56, 'n_a': 103, 'n_steps': 6, 'gamma': 1.8777450385582528, 'lambda_sparse': 0.0005860698746981756, 'learning_rate': 0.005401089622465986, 'batch_size': 256}. Best is trial 12 with value: 0.9482014388489208.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 53 with best_epoch = 33 and best_valid_accuracy = 0.9223\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-03-01 05:49:08,146] Trial 18 finished with value: 0.9223021582733812 and parameters: {'n_d': 70, 'n_a': 51, 'n_steps': 9, 'gamma': 1.6955828761262328, 'lambda_sparse': 0.0002793117285600803, 'learning_rate': 0.051021863743632934, 'batch_size': 128}. Best is trial 12 with value: 0.9482014388489208.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 68 with best_epoch = 48 and best_valid_accuracy = 0.93525\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-03-01 05:49:37,690] Trial 19 finished with value: 0.935251798561151 and parameters: {'n_d': 80, 'n_a': 94, 'n_steps': 4, 'gamma': 1.57085196809894, 'lambda_sparse': 0.0019065623341612654, 'learning_rate': 0.022419354738293533, 'batch_size': 256}. Best is trial 12 with value: 0.9482014388489208.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 95 with best_epoch = 75 and best_valid_accuracy = 0.94676\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-03-01 05:50:32,955] Trial 20 finished with value: 0.9467625899280575 and parameters: {'n_d': 104, 'n_a': 114, 'n_steps': 6, 'gamma': 1.1110246351971762, 'lambda_sparse': 0.0036641569166262398, 'learning_rate': 0.018204795634213688, 'batch_size': 256}. Best is trial 12 with value: 0.9482014388489208.\n",
            "[I 2025-03-01 05:50:54,906] Trial 21 finished with value: 0.9381294964028777 and parameters: {'n_d': 100, 'n_a': 35, 'n_steps': 3, 'gamma': 1.3349825893672285, 'lambda_sparse': 0.0014804490218860349, 'learning_rate': 0.01528083040342535, 'batch_size': 256}. Best is trial 12 with value: 0.9482014388489208.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 63 with best_epoch = 43 and best_valid_accuracy = 0.93813\n",
            "\n",
            "Early stopping occurred at epoch 54 with best_epoch = 34 and best_valid_accuracy = 0.9223\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-03-01 05:51:17,801] Trial 22 finished with value: 0.9223021582733812 and parameters: {'n_d': 80, 'n_a': 44, 'n_steps': 4, 'gamma': 1.189458441773203, 'lambda_sparse': 0.0008222604418608145, 'learning_rate': 0.007167264971724088, 'batch_size': 256}. Best is trial 12 with value: 0.9482014388489208.\n",
            "[I 2025-03-01 05:51:41,860] Trial 23 finished with value: 0.9381294964028777 and parameters: {'n_d': 105, 'n_a': 59, 'n_steps': 3, 'gamma': 1.4029500916395077, 'lambda_sparse': 0.0008537870529334091, 'learning_rate': 0.05377357218515587, 'batch_size': 256}. Best is trial 12 with value: 0.9482014388489208.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 70 with best_epoch = 50 and best_valid_accuracy = 0.93813\n",
            "\n",
            "Early stopping occurred at epoch 58 with best_epoch = 38 and best_valid_accuracy = 0.92374\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-03-01 05:52:11,397] Trial 24 finished with value: 0.9237410071942446 and parameters: {'n_d': 127, 'n_a': 42, 'n_steps': 5, 'gamma': 1.4580194825712287, 'lambda_sparse': 0.0014388604490439228, 'learning_rate': 0.017146308533032506, 'batch_size': 256}. Best is trial 12 with value: 0.9482014388489208.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 56 with best_epoch = 36 and best_valid_accuracy = 0.9295\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-03-01 05:52:53,285] Trial 25 finished with value: 0.9294964028776979 and parameters: {'n_d': 65, 'n_a': 42, 'n_steps': 4, 'gamma': 1.3059559258472329, 'lambda_sparse': 0.003456450914494032, 'learning_rate': 0.03402653844344627, 'batch_size': 128}. Best is trial 12 with value: 0.9482014388489208.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 67 with best_epoch = 47 and best_valid_accuracy = 0.90791\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-03-01 05:53:32,182] Trial 26 finished with value: 0.9079136690647482 and parameters: {'n_d': 51, 'n_a': 32, 'n_steps': 6, 'gamma': 1.891014519601094, 'lambda_sparse': 0.00042158754124955593, 'learning_rate': 0.009897063400083977, 'batch_size': 256}. Best is trial 12 with value: 0.9482014388489208.\n",
            "[I 2025-03-01 05:54:02,541] Trial 27 finished with value: 0.9453237410071943 and parameters: {'n_d': 84, 'n_a': 60, 'n_steps': 3, 'gamma': 1.602268878675042, 'lambda_sparse': 0.0016171238565492168, 'learning_rate': 0.021547862908002233, 'batch_size': 256}. Best is trial 12 with value: 0.9482014388489208.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 90 with best_epoch = 70 and best_valid_accuracy = 0.94532\n",
            "\n",
            "Early stopping occurred at epoch 89 with best_epoch = 69 and best_valid_accuracy = 0.92662\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-03-01 05:55:15,144] Trial 28 finished with value: 0.9266187050359712 and parameters: {'n_d': 96, 'n_a': 94, 'n_steps': 9, 'gamma': 1.7303404743381579, 'lambda_sparse': 0.0008190097871612558, 'learning_rate': 0.013040530520969684, 'batch_size': 256}. Best is trial 12 with value: 0.9482014388489208.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 61 with best_epoch = 41 and best_valid_accuracy = 0.93813\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-03-01 05:56:35,559] Trial 29 finished with value: 0.9381294964028777 and parameters: {'n_d': 108, 'n_a': 52, 'n_steps': 8, 'gamma': 1.2276395709485195, 'lambda_sparse': 0.0001903542795017346, 'learning_rate': 0.027035724056136703, 'batch_size': 128}. Best is trial 12 with value: 0.9482014388489208.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 64 with best_epoch = 44 and best_valid_accuracy = 0.94101\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-03-01 05:57:03,214] Trial 30 finished with value: 0.9410071942446043 and parameters: {'n_d': 76, 'n_a': 128, 'n_steps': 4, 'gamma': 1.3589239214669202, 'lambda_sparse': 0.0045147497080449545, 'learning_rate': 0.00503408592496595, 'batch_size': 256}. Best is trial 12 with value: 0.9482014388489208.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stop training because you reached max_epochs = 100 with best_epoch = 90 and best_valid_accuracy = 0.94532\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-03-01 05:58:01,321] Trial 31 finished with value: 0.9453237410071943 and parameters: {'n_d': 106, 'n_a': 116, 'n_steps': 6, 'gamma': 1.096073148695938, 'lambda_sparse': 0.003411577376532353, 'learning_rate': 0.01877634396489933, 'batch_size': 256}. Best is trial 12 with value: 0.9482014388489208.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 71 with best_epoch = 51 and best_valid_accuracy = 0.94101\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-03-01 05:58:48,223] Trial 32 finished with value: 0.9410071942446043 and parameters: {'n_d': 100, 'n_a': 115, 'n_steps': 7, 'gamma': 1.1061603786327756, 'lambda_sparse': 0.002024688613321335, 'learning_rate': 0.043889955495168616, 'batch_size': 256}. Best is trial 12 with value: 0.9482014388489208.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 82 with best_epoch = 62 and best_valid_accuracy = 0.94245\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-03-01 05:59:29,310] Trial 33 finished with value: 0.9424460431654677 and parameters: {'n_d': 87, 'n_a': 111, 'n_steps': 5, 'gamma': 1.169539805898824, 'lambda_sparse': 0.004026871862489076, 'learning_rate': 0.029292675237687166, 'batch_size': 256}. Best is trial 12 with value: 0.9482014388489208.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 85 with best_epoch = 65 and best_valid_accuracy = 0.9223\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-03-01 06:00:18,534] Trial 34 finished with value: 0.9223021582733812 and parameters: {'n_d': 67, 'n_a': 100, 'n_steps': 6, 'gamma': 1.4825990523215373, 'lambda_sparse': 0.008393173213857501, 'learning_rate': 0.010488175236177467, 'batch_size': 256}. Best is trial 12 with value: 0.9482014388489208.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stop training because you reached max_epochs = 100 with best_epoch = 91 and best_valid_accuracy = 0.9482\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-03-01 06:01:01,995] Trial 35 finished with value: 0.9482014388489208 and parameters: {'n_d': 127, 'n_a': 78, 'n_steps': 7, 'gamma': 1.0935183107679007, 'lambda_sparse': 0.005607343256354971, 'learning_rate': 0.017203598142651978, 'batch_size': 512}. Best is trial 12 with value: 0.9482014388489208.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stop training because you reached max_epochs = 100 with best_epoch = 84 and best_valid_accuracy = 0.94388\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-03-01 06:01:50,519] Trial 36 finished with value: 0.943884892086331 and parameters: {'n_d': 128, 'n_a': 47, 'n_steps': 8, 'gamma': 1.0256674944992834, 'lambda_sparse': 0.006889211735659215, 'learning_rate': 0.013851617419082342, 'batch_size': 512}. Best is trial 12 with value: 0.9482014388489208.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stop training because you reached max_epochs = 100 with best_epoch = 87 and best_valid_accuracy = 0.8964\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-03-01 06:02:33,939] Trial 37 finished with value: 0.8964028776978418 and parameters: {'n_d': 118, 'n_a': 85, 'n_steps': 7, 'gamma': 1.6239580702531322, 'lambda_sparse': 0.005306693774595824, 'learning_rate': 0.008710136236495126, 'batch_size': 512}. Best is trial 12 with value: 0.9482014388489208.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stop training because you reached max_epochs = 100 with best_epoch = 94 and best_valid_accuracy = 0.93669\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-03-01 06:03:16,861] Trial 38 finished with value: 0.9366906474820144 and parameters: {'n_d': 112, 'n_a': 76, 'n_steps': 7, 'gamma': 1.5358181264844626, 'lambda_sparse': 0.001064210023499235, 'learning_rate': 0.02123576143599012, 'batch_size': 512}. Best is trial 12 with value: 0.9482014388489208.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stop training because you reached max_epochs = 100 with best_epoch = 86 and best_valid_accuracy = 0.91223\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-03-01 06:04:05,336] Trial 39 finished with value: 0.9122302158273381 and parameters: {'n_d': 123, 'n_a': 62, 'n_steps': 8, 'gamma': 1.744257814185068, 'lambda_sparse': 0.0003869943818516412, 'learning_rate': 0.004080208482668854, 'batch_size': 512}. Best is trial 12 with value: 0.9482014388489208.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stop training because you reached max_epochs = 100 with best_epoch = 90 and best_valid_accuracy = 0.9295\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-03-01 06:04:59,556] Trial 40 finished with value: 0.9294964028776979 and parameters: {'n_d': 51, 'n_a': 87, 'n_steps': 9, 'gamma': 1.9460872048355529, 'lambda_sparse': 0.0026648224639779622, 'learning_rate': 0.011917449802311204, 'batch_size': 512}. Best is trial 12 with value: 0.9482014388489208.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stop training because you reached max_epochs = 100 with best_epoch = 90 and best_valid_accuracy = 0.93957\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-03-01 06:05:43,208] Trial 41 finished with value: 0.939568345323741 and parameters: {'n_d': 113, 'n_a': 78, 'n_steps': 7, 'gamma': 1.1040150867611835, 'lambda_sparse': 0.004395756963682241, 'learning_rate': 0.017657345330279937, 'batch_size': 512}. Best is trial 12 with value: 0.9482014388489208.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 93 with best_epoch = 73 and best_valid_accuracy = 0.93669\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-03-01 06:06:37,681] Trial 42 finished with value: 0.9366906474820144 and parameters: {'n_d': 62, 'n_a': 39, 'n_steps': 6, 'gamma': 1.270891240561045, 'lambda_sparse': 0.003268816698760669, 'learning_rate': 0.02471049420598083, 'batch_size': 256}. Best is trial 12 with value: 0.9482014388489208.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 68 with best_epoch = 48 and best_valid_accuracy = 0.93813\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-03-01 06:07:12,682] Trial 43 finished with value: 0.9381294964028777 and parameters: {'n_d': 75, 'n_a': 111, 'n_steps': 5, 'gamma': 1.1495185728487205, 'lambda_sparse': 0.007169501568029148, 'learning_rate': 0.03433081749751209, 'batch_size': 256}. Best is trial 12 with value: 0.9482014388489208.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stop training because you reached max_epochs = 100 with best_epoch = 88 and best_valid_accuracy = 0.9482\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-03-01 06:08:01,454] Trial 44 finished with value: 0.9482014388489208 and parameters: {'n_d': 120, 'n_a': 67, 'n_steps': 8, 'gamma': 1.0476052843886314, 'lambda_sparse': 0.0052926154859915175, 'learning_rate': 0.01620057054360051, 'batch_size': 512}. Best is trial 12 with value: 0.9482014388489208.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 87 with best_epoch = 67 and best_valid_accuracy = 0.93669\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-03-01 06:08:44,758] Trial 45 finished with value: 0.9366906474820144 and parameters: {'n_d': 122, 'n_a': 68, 'n_steps': 8, 'gamma': 1.0126540437711897, 'lambda_sparse': 0.005337271261262433, 'learning_rate': 0.01521481843267545, 'batch_size': 512}. Best is trial 12 with value: 0.9482014388489208.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stop training because you reached max_epochs = 100 with best_epoch = 99 and best_valid_accuracy = 0.94101\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-03-01 06:09:28,653] Trial 46 finished with value: 0.9410071942446043 and parameters: {'n_d': 118, 'n_a': 71, 'n_steps': 7, 'gamma': 1.0605522056681387, 'lambda_sparse': 0.0006821819620452867, 'learning_rate': 0.007467923079831372, 'batch_size': 512}. Best is trial 12 with value: 0.9482014388489208.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stop training because you reached max_epochs = 100 with best_epoch = 88 and best_valid_accuracy = 0.91942\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-03-01 06:10:22,917] Trial 47 finished with value: 0.9194244604316547 and parameters: {'n_d': 124, 'n_a': 55, 'n_steps': 9, 'gamma': 1.2356442602805082, 'lambda_sparse': 0.001267681149325903, 'learning_rate': 0.0020428451670107175, 'batch_size': 512}. Best is trial 12 with value: 0.9482014388489208.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 98 with best_epoch = 78 and best_valid_accuracy = 0.9223\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-03-01 06:11:11,606] Trial 48 finished with value: 0.9223021582733812 and parameters: {'n_d': 111, 'n_a': 64, 'n_steps': 8, 'gamma': 1.9283504382850438, 'lambda_sparse': 0.002359024790218105, 'learning_rate': 0.030123677898072236, 'batch_size': 512}. Best is trial 12 with value: 0.9482014388489208.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stop training because you reached max_epochs = 100 with best_epoch = 82 and best_valid_accuracy = 0.94964\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-03-01 06:11:55,603] Trial 49 finished with value: 0.9496402877697842 and parameters: {'n_d': 95, 'n_a': 82, 'n_steps': 7, 'gamma': 1.0604035581458195, 'lambda_sparse': 0.00023309579931048954, 'learning_rate': 0.06385200251604904, 'batch_size': 512}. Best is trial 49 with value: 0.9496402877697842.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Hyperparameters:  {'n_d': 95, 'n_a': 82, 'n_steps': 7, 'gamma': 1.0604035581458195, 'lambda_sparse': 0.00023309579931048954, 'learning_rate': 0.06385200251604904, 'batch_size': 512}\n",
            "Best Validation Accuracy:  0.9496402877697842\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------\n",
        "# Part 1: Data Augmentation Function\n",
        "# -------------------\n",
        "def augment_data(X, y, permutation_prob=0.1):\n",
        "    \"\"\"\n",
        "    Augment the dataset by randomly permuting feature orders with a given probability.\n",
        "\n",
        "    Parameters:\n",
        "    - X (numpy.ndarray or pandas.DataFrame): Feature matrix.\n",
        "    - y (numpy.ndarray or pandas.Series): Target vector.\n",
        "    - permutation_prob (float): Probability of permuting each sample.\n",
        "\n",
        "    Returns:\n",
        "    - X_augmented (numpy.ndarray): Augmented feature matrix.\n",
        "    - y_augmented (numpy.ndarray): Augmented target vector.\n",
        "    \"\"\"\n",
        "    X_augmented = []\n",
        "    y_augmented = []\n",
        "    for sample, label in zip(X, y):\n",
        "        if np.random.rand() < permutation_prob:\n",
        "            perm = np.random.permutation(sample.shape[0])\n",
        "            sample = sample[perm]\n",
        "        X_augmented.append(sample)\n",
        "        y_augmented.append(label)\n",
        "    return np.array(X_augmented), np.array(y_augmented)\n",
        "\n",
        "# -------------------\n",
        "# Part 2: Apply Data Augmentation\n",
        "# -------------------\n",
        "permutation_probability = 0.1\n",
        "X_train_augmented, y_train_augmented = augment_data(\n",
        "    X_train_final.values,\n",
        "    y_train_final.values,\n",
        "    permutation_prob=permutation_probability\n",
        ")\n",
        "\n",
        "print(f\"Original Training Set Shape: {X_train_final.shape}\")\n",
        "print(f\"Augmented Training Set Shape: {X_train_augmented.shape}\")\n",
        "\n",
        "# -------------------\n",
        "# Part 3: Initialize and Train TabNet with Augmented Data\n",
        "# -------------------\n",
        "perm_reg_tabnet = TabNetClassifier(\n",
        "    input_dim=X_train_final.shape[1],    # Number of features (10)\n",
        "    output_dim=3,                        # Number of classes: Normal, Suspect, Pathological\n",
        "    n_d=study.best_params['n_d'],\n",
        "    n_a=study.best_params['n_a'],\n",
        "    n_steps=study.best_params['n_steps'],\n",
        "    gamma=study.best_params['gamma'],\n",
        "    lambda_sparse=study.best_params['lambda_sparse'],\n",
        "    optimizer_fn=torch.optim.Adam,\n",
        "    optimizer_params={'lr': study.best_params['learning_rate']},\n",
        "    mask_type='sparsemax',\n",
        "    verbose=1,\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "perm_reg_tabnet.fit(\n",
        "    X_train=X_train_augmented,\n",
        "    y_train=y_train_augmented,\n",
        "    eval_set=[(X_valid.values, y_valid.values), (X_test_scaled.values, y_test.values)],\n",
        "    eval_name=['train', 'valid'],\n",
        "    eval_metric=['accuracy'],\n",
        "    max_epochs=100,\n",
        "    patience=20,\n",
        "    batch_size=study.best_params['batch_size'],\n",
        "    virtual_batch_size=128,\n",
        "    num_workers=0,\n",
        "    drop_last=False\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9i0WttyXMFrU",
        "outputId": "8c7e97d0-f030-45d6-9b2a-a8dcadd44f9c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Training Set Shape: (2780, 10)\n",
            "Augmented Training Set Shape: (2780, 10)\n",
            "epoch 0  | loss: 3.14036 | train_accuracy: 0.34676 | valid_accuracy: 0.34631 |  0:00:00s\n",
            "epoch 1  | loss: 2.06169 | train_accuracy: 0.41439 | valid_accuracy: 0.42215 |  0:00:01s\n",
            "epoch 2  | loss: 0.857   | train_accuracy: 0.39856 | valid_accuracy: 0.4255  |  0:00:01s\n",
            "epoch 3  | loss: 0.59457 | train_accuracy: 0.52662 | valid_accuracy: 0.54698 |  0:00:02s\n",
            "epoch 4  | loss: 0.55823 | train_accuracy: 0.71511 | valid_accuracy: 0.72416 |  0:00:02s\n",
            "epoch 5  | loss: 0.56698 | train_accuracy: 0.57842 | valid_accuracy: 0.5906  |  0:00:03s\n",
            "epoch 6  | loss: 0.50694 | train_accuracy: 0.61727 | valid_accuracy: 0.6255  |  0:00:03s\n",
            "epoch 7  | loss: 0.4583  | train_accuracy: 0.76691 | valid_accuracy: 0.73893 |  0:00:04s\n",
            "epoch 8  | loss: 0.43688 | train_accuracy: 0.70504 | valid_accuracy: 0.68658 |  0:00:04s\n",
            "epoch 9  | loss: 0.43313 | train_accuracy: 0.77986 | valid_accuracy: 0.74832 |  0:00:05s\n",
            "epoch 10 | loss: 0.43383 | train_accuracy: 0.65899 | valid_accuracy: 0.6651  |  0:00:05s\n",
            "epoch 11 | loss: 0.41572 | train_accuracy: 0.75252 | valid_accuracy: 0.76107 |  0:00:06s\n",
            "epoch 12 | loss: 0.41804 | train_accuracy: 0.77266 | valid_accuracy: 0.75772 |  0:00:06s\n",
            "epoch 13 | loss: 0.40461 | train_accuracy: 0.76403 | valid_accuracy: 0.74698 |  0:00:07s\n",
            "epoch 14 | loss: 0.39998 | train_accuracy: 0.71511 | valid_accuracy: 0.7255  |  0:00:07s\n",
            "epoch 15 | loss: 0.37535 | train_accuracy: 0.72662 | valid_accuracy: 0.72349 |  0:00:08s\n",
            "epoch 16 | loss: 0.39262 | train_accuracy: 0.7223  | valid_accuracy: 0.71611 |  0:00:08s\n",
            "epoch 17 | loss: 0.39031 | train_accuracy: 0.70504 | valid_accuracy: 0.70872 |  0:00:09s\n",
            "epoch 18 | loss: 0.3847  | train_accuracy: 0.67338 | valid_accuracy: 0.68993 |  0:00:10s\n",
            "epoch 19 | loss: 0.37681 | train_accuracy: 0.64748 | valid_accuracy: 0.64765 |  0:00:10s\n",
            "epoch 20 | loss: 0.39756 | train_accuracy: 0.76115 | valid_accuracy: 0.75101 |  0:00:11s\n",
            "epoch 21 | loss: 0.39181 | train_accuracy: 0.71079 | valid_accuracy: 0.69799 |  0:00:12s\n",
            "epoch 22 | loss: 0.38762 | train_accuracy: 0.7554  | valid_accuracy: 0.75705 |  0:00:12s\n",
            "epoch 23 | loss: 0.36779 | train_accuracy: 0.77266 | valid_accuracy: 0.77248 |  0:00:13s\n",
            "epoch 24 | loss: 0.36906 | train_accuracy: 0.69209 | valid_accuracy: 0.69933 |  0:00:13s\n",
            "epoch 25 | loss: 0.35781 | train_accuracy: 0.73094 | valid_accuracy: 0.73423 |  0:00:14s\n",
            "epoch 26 | loss: 0.36479 | train_accuracy: 0.63453 | valid_accuracy: 0.63624 |  0:00:14s\n",
            "epoch 27 | loss: 0.35486 | train_accuracy: 0.72374 | valid_accuracy: 0.72752 |  0:00:15s\n",
            "epoch 28 | loss: 0.35019 | train_accuracy: 0.76259 | valid_accuracy: 0.75973 |  0:00:15s\n",
            "epoch 29 | loss: 0.34463 | train_accuracy: 0.75971 | valid_accuracy: 0.73423 |  0:00:16s\n",
            "epoch 30 | loss: 0.35022 | train_accuracy: 0.73813 | valid_accuracy: 0.7302  |  0:00:16s\n",
            "epoch 31 | loss: 0.33436 | train_accuracy: 0.79712 | valid_accuracy: 0.79799 |  0:00:17s\n",
            "epoch 32 | loss: 0.33955 | train_accuracy: 0.71223 | valid_accuracy: 0.70067 |  0:00:17s\n",
            "epoch 33 | loss: 0.37638 | train_accuracy: 0.79281 | valid_accuracy: 0.78456 |  0:00:18s\n",
            "epoch 34 | loss: 0.35435 | train_accuracy: 0.72086 | valid_accuracy: 0.72215 |  0:00:18s\n",
            "epoch 35 | loss: 0.35126 | train_accuracy: 0.77842 | valid_accuracy: 0.7698  |  0:00:19s\n",
            "epoch 36 | loss: 0.32046 | train_accuracy: 0.85899 | valid_accuracy: 0.84295 |  0:00:19s\n",
            "epoch 37 | loss: 0.32249 | train_accuracy: 0.74964 | valid_accuracy: 0.74832 |  0:00:20s\n",
            "epoch 38 | loss: 0.33776 | train_accuracy: 0.82302 | valid_accuracy: 0.81611 |  0:00:20s\n",
            "epoch 39 | loss: 0.32821 | train_accuracy: 0.83165 | valid_accuracy: 0.8094  |  0:00:21s\n",
            "epoch 40 | loss: 0.3111  | train_accuracy: 0.83597 | valid_accuracy: 0.81208 |  0:00:21s\n",
            "epoch 41 | loss: 0.33088 | train_accuracy: 0.88489 | valid_accuracy: 0.85906 |  0:00:22s\n",
            "epoch 42 | loss: 0.33996 | train_accuracy: 0.84604 | valid_accuracy: 0.82752 |  0:00:23s\n",
            "epoch 43 | loss: 0.34595 | train_accuracy: 0.85899 | valid_accuracy: 0.85772 |  0:00:23s\n",
            "epoch 44 | loss: 0.36522 | train_accuracy: 0.77266 | valid_accuracy: 0.75369 |  0:00:24s\n",
            "epoch 45 | loss: 0.36288 | train_accuracy: 0.82734 | valid_accuracy: 0.80671 |  0:00:24s\n",
            "epoch 46 | loss: 0.36583 | train_accuracy: 0.80144 | valid_accuracy: 0.78792 |  0:00:25s\n",
            "epoch 47 | loss: 0.3575  | train_accuracy: 0.89065 | valid_accuracy: 0.87315 |  0:00:25s\n",
            "epoch 48 | loss: 0.34365 | train_accuracy: 0.87914 | valid_accuracy: 0.85772 |  0:00:26s\n",
            "epoch 49 | loss: 0.34643 | train_accuracy: 0.84173 | valid_accuracy: 0.83154 |  0:00:26s\n",
            "epoch 50 | loss: 0.30708 | train_accuracy: 0.86043 | valid_accuracy: 0.8604  |  0:00:27s\n",
            "epoch 51 | loss: 0.31357 | train_accuracy: 0.9036  | valid_accuracy: 0.88725 |  0:00:27s\n",
            "epoch 52 | loss: 0.32892 | train_accuracy: 0.89353 | valid_accuracy: 0.88389 |  0:00:28s\n",
            "epoch 53 | loss: 0.31277 | train_accuracy: 0.87626 | valid_accuracy: 0.85168 |  0:00:28s\n",
            "epoch 54 | loss: 0.31888 | train_accuracy: 0.89353 | valid_accuracy: 0.89329 |  0:00:29s\n",
            "epoch 55 | loss: 0.31473 | train_accuracy: 0.91079 | valid_accuracy: 0.88523 |  0:00:29s\n",
            "epoch 56 | loss: 0.33605 | train_accuracy: 0.87914 | valid_accuracy: 0.8745  |  0:00:30s\n",
            "epoch 57 | loss: 0.32228 | train_accuracy: 0.90504 | valid_accuracy: 0.88523 |  0:00:30s\n",
            "epoch 58 | loss: 0.31053 | train_accuracy: 0.89353 | valid_accuracy: 0.88322 |  0:00:31s\n",
            "epoch 59 | loss: 0.31043 | train_accuracy: 0.86475 | valid_accuracy: 0.87718 |  0:00:31s\n",
            "epoch 60 | loss: 0.32836 | train_accuracy: 0.88777 | valid_accuracy: 0.86309 |  0:00:32s\n",
            "epoch 61 | loss: 0.29889 | train_accuracy: 0.88633 | valid_accuracy: 0.88658 |  0:00:32s\n",
            "epoch 62 | loss: 0.28837 | train_accuracy: 0.89496 | valid_accuracy: 0.8906  |  0:00:33s\n",
            "epoch 63 | loss: 0.29381 | train_accuracy: 0.90072 | valid_accuracy: 0.8906  |  0:00:33s\n",
            "epoch 64 | loss: 0.29185 | train_accuracy: 0.87194 | valid_accuracy: 0.87315 |  0:00:34s\n",
            "epoch 65 | loss: 0.2922  | train_accuracy: 0.91655 | valid_accuracy: 0.91745 |  0:00:35s\n",
            "epoch 66 | loss: 0.28428 | train_accuracy: 0.89928 | valid_accuracy: 0.89329 |  0:00:35s\n",
            "epoch 67 | loss: 0.28591 | train_accuracy: 0.90216 | valid_accuracy: 0.89732 |  0:00:36s\n",
            "epoch 68 | loss: 0.2941  | train_accuracy: 0.88201 | valid_accuracy: 0.8651  |  0:00:36s\n",
            "epoch 69 | loss: 0.28123 | train_accuracy: 0.90935 | valid_accuracy: 0.89933 |  0:00:37s\n",
            "epoch 70 | loss: 0.28177 | train_accuracy: 0.90647 | valid_accuracy: 0.90336 |  0:00:37s\n",
            "epoch 71 | loss: 0.26387 | train_accuracy: 0.91367 | valid_accuracy: 0.90738 |  0:00:38s\n",
            "epoch 72 | loss: 0.26346 | train_accuracy: 0.89209 | valid_accuracy: 0.90201 |  0:00:38s\n",
            "epoch 73 | loss: 0.2555  | train_accuracy: 0.89209 | valid_accuracy: 0.8953  |  0:00:39s\n",
            "epoch 74 | loss: 0.26591 | train_accuracy: 0.91367 | valid_accuracy: 0.91409 |  0:00:39s\n",
            "epoch 75 | loss: 0.25181 | train_accuracy: 0.90072 | valid_accuracy: 0.90671 |  0:00:40s\n",
            "epoch 76 | loss: 0.25864 | train_accuracy: 0.91655 | valid_accuracy: 0.90067 |  0:00:40s\n",
            "epoch 77 | loss: 0.2391  | train_accuracy: 0.90935 | valid_accuracy: 0.90872 |  0:00:41s\n",
            "epoch 78 | loss: 0.22058 | train_accuracy: 0.91799 | valid_accuracy: 0.92685 |  0:00:41s\n",
            "epoch 79 | loss: 0.21068 | train_accuracy: 0.92662 | valid_accuracy: 0.93221 |  0:00:42s\n",
            "epoch 80 | loss: 0.21489 | train_accuracy: 0.9295  | valid_accuracy: 0.92148 |  0:00:42s\n",
            "epoch 81 | loss: 0.20714 | train_accuracy: 0.91655 | valid_accuracy: 0.91745 |  0:00:43s\n",
            "epoch 82 | loss: 0.21032 | train_accuracy: 0.91367 | valid_accuracy: 0.90201 |  0:00:43s\n",
            "epoch 83 | loss: 0.20309 | train_accuracy: 0.91079 | valid_accuracy: 0.90805 |  0:00:44s\n",
            "epoch 84 | loss: 0.22853 | train_accuracy: 0.90935 | valid_accuracy: 0.89732 |  0:00:44s\n",
            "epoch 85 | loss: 0.22326 | train_accuracy: 0.9036  | valid_accuracy: 0.90268 |  0:00:45s\n",
            "epoch 86 | loss: 0.21654 | train_accuracy: 0.90791 | valid_accuracy: 0.91208 |  0:00:45s\n",
            "epoch 87 | loss: 0.20296 | train_accuracy: 0.90504 | valid_accuracy: 0.91745 |  0:00:46s\n",
            "epoch 88 | loss: 0.18836 | train_accuracy: 0.9223  | valid_accuracy: 0.91812 |  0:00:47s\n",
            "epoch 89 | loss: 0.18971 | train_accuracy: 0.91223 | valid_accuracy: 0.92148 |  0:00:48s\n",
            "epoch 90 | loss: 0.18821 | train_accuracy: 0.88921 | valid_accuracy: 0.90537 |  0:00:48s\n",
            "epoch 91 | loss: 0.19504 | train_accuracy: 0.91511 | valid_accuracy: 0.91544 |  0:00:49s\n",
            "epoch 92 | loss: 0.19714 | train_accuracy: 0.90791 | valid_accuracy: 0.91477 |  0:00:49s\n",
            "epoch 93 | loss: 0.18822 | train_accuracy: 0.90935 | valid_accuracy: 0.9094  |  0:00:50s\n",
            "epoch 94 | loss: 0.18723 | train_accuracy: 0.9223  | valid_accuracy: 0.91745 |  0:00:50s\n",
            "epoch 95 | loss: 0.18307 | train_accuracy: 0.90647 | valid_accuracy: 0.91275 |  0:00:51s\n",
            "epoch 96 | loss: 0.18774 | train_accuracy: 0.92518 | valid_accuracy: 0.91611 |  0:00:51s\n",
            "epoch 97 | loss: 0.20529 | train_accuracy: 0.93094 | valid_accuracy: 0.92148 |  0:00:52s\n",
            "epoch 98 | loss: 0.17615 | train_accuracy: 0.90647 | valid_accuracy: 0.90336 |  0:00:52s\n",
            "epoch 99 | loss: 0.17874 | train_accuracy: 0.89209 | valid_accuracy: 0.90201 |  0:00:53s\n",
            "\n",
            "Early stopping occurred at epoch 99 with best_epoch = 79 and best_valid_accuracy = 0.93221\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------\n",
        "# Part 4: Predict and Evaluate on the Test Set\n",
        "# -------------------\n",
        "y_pred_perm_reg = perm_reg_tabnet.predict(X_test_scaled.values)\n",
        "\n",
        "print(\"\\nPermutation Regularized TabNet Classification Report:\")\n",
        "print(classification_report(y_test, y_pred_perm_reg, target_names=['Normal', 'Suspect', 'Pathological']))\n",
        "\n",
        "conf_matrix = confusion_matrix(y_test, y_pred_perm_reg)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=['Normal', 'Suspect', 'Pathological'],\n",
        "            yticklabels=['Normal', 'Suspect', 'Pathological'])\n",
        "plt.title('Confusion Matrix for Permutation Regularized TabNet')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 778
        },
        "id": "KOMtecwkMFty",
        "outputId": "5b01e1c2-a50e-465d-ec68-a3689927f2a5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Permutation Regularized TabNet Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      Normal       0.94      0.93      0.94       496\n",
            "     Suspect       0.88      0.95      0.92       497\n",
            "Pathological       0.98      0.91      0.94       497\n",
            "\n",
            "    accuracy                           0.93      1490\n",
            "   macro avg       0.93      0.93      0.93      1490\n",
            "weighted avg       0.93      0.93      0.93      1490\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4kJORIpYMFwQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LBa8YQX_MFye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "si0ZUmfwMF00"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-Ru9YeneC49P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oTcx7XUGC4_e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Jl3CakqjC5Bb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dlVIEJEbC5Dq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}