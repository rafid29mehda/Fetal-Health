{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SVYYYotl7R5X",
        "outputId": "5de2aa69-4ebe-48a9-f08f-0aadab0dff02"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pytorch-tabnet in /usr/local/lib/python3.10/dist-packages (4.1.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from pytorch-tabnet) (1.26.4)\n",
            "Requirement already satisfied: scikit_learn>0.21 in /usr/local/lib/python3.10/dist-packages (from pytorch-tabnet) (1.6.0)\n",
            "Requirement already satisfied: scipy>1.4 in /usr/local/lib/python3.10/dist-packages (from pytorch-tabnet) (1.13.1)\n",
            "Requirement already satisfied: torch>=1.3 in /usr/local/lib/python3.10/dist-packages (from pytorch-tabnet) (2.5.1+cu121)\n",
            "Requirement already satisfied: tqdm>=4.36 in /usr/local/lib/python3.10/dist-packages (from pytorch-tabnet) (4.67.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit_learn>0.21->pytorch-tabnet) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit_learn>0.21->pytorch-tabnet) (3.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->pytorch-tabnet) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->pytorch-tabnet) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->pytorch-tabnet) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->pytorch-tabnet) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->pytorch-tabnet) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->pytorch-tabnet) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.3->pytorch-tabnet) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.3->pytorch-tabnet) (3.0.2)\n",
            "Requirement already satisfied: captum in /usr/local/lib/python3.10/dist-packages (0.7.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from captum) (3.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from captum) (1.26.4)\n",
            "Requirement already satisfied: torch>=1.6 in /usr/local/lib/python3.10/dist-packages (from captum) (2.5.1+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from captum) (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->captum) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->captum) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->captum) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->captum) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->captum) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->captum) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.6->captum) (1.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (4.55.3)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->captum) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6->captum) (3.0.2)\n",
            "Requirement already satisfied: optuna in /usr/local/lib/python3.10/dist-packages (4.1.0)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (1.14.0)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.10/dist-packages (from optuna) (6.9.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (24.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.36)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna) (6.0.2)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (1.3.8)\n",
            "Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (4.12.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.1.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.2)\n",
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.10/dist-packages (0.13.0)\n",
            "Requirement already satisfied: numpy<3,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy<2,>=1.10.1 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn<2,>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.6.0)\n",
            "Requirement already satisfied: sklearn-compat<1,>=0.1 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (0.1.3)\n",
            "Requirement already satisfied: joblib<2,>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl<4,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (3.5.0)\n",
            "Requirement already satisfied: dask-expr in /usr/local/lib/python3.10/dist-packages (1.1.21)\n",
            "Requirement already satisfied: dask==2024.12.1 in /usr/local/lib/python3.10/dist-packages (from dask-expr) (2024.12.1)\n",
            "Requirement already satisfied: pyarrow>=14.0.1 in /usr/local/lib/python3.10/dist-packages (from dask-expr) (17.0.0)\n",
            "Requirement already satisfied: pandas>=2 in /usr/local/lib/python3.10/dist-packages (from dask-expr) (2.2.2)\n",
            "Requirement already satisfied: click>=8.1 in /usr/local/lib/python3.10/dist-packages (from dask==2024.12.1->dask-expr) (8.1.8)\n",
            "Requirement already satisfied: cloudpickle>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from dask==2024.12.1->dask-expr) (3.1.0)\n",
            "Requirement already satisfied: fsspec>=2021.09.0 in /usr/local/lib/python3.10/dist-packages (from dask==2024.12.1->dask-expr) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from dask==2024.12.1->dask-expr) (24.2)\n",
            "Requirement already satisfied: partd>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from dask==2024.12.1->dask-expr) (1.4.2)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from dask==2024.12.1->dask-expr) (6.0.2)\n",
            "Requirement already satisfied: toolz>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from dask==2024.12.1->dask-expr) (0.12.1)\n",
            "Requirement already satisfied: importlib_metadata>=4.13.0 in /usr/local/lib/python3.10/dist-packages (from dask==2024.12.1->dask-expr) (8.5.0)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas>=2->dask-expr) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=2->dask-expr) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2->dask-expr) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=2->dask-expr) (2024.2)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib_metadata>=4.13.0->dask==2024.12.1->dask-expr) (3.21.0)\n",
            "Requirement already satisfied: locket in /usr/local/lib/python3.10/dist-packages (from partd>=1.4.0->dask==2024.12.1->dask-expr) (1.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=2->dask-expr) (1.17.0)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement scikit-learn-contrib (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for scikit-learn-contrib\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: lightgbm in /usr/local/lib/python3.10/dist-packages (4.5.0)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from lightgbm) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from lightgbm) (1.13.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-01-12 07:05:55,243] A new study created in memory with name: no-name-739949e5-f554-43ec-8096-7d7fbfac204c\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First five rows of the dataset:\n",
            "   baseline value  accelerations  fetal_movement  uterine_contractions  \\\n",
            "0           120.0          0.000             0.0                 0.000   \n",
            "1           132.0          0.006             0.0                 0.006   \n",
            "2           133.0          0.003             0.0                 0.008   \n",
            "3           134.0          0.003             0.0                 0.008   \n",
            "4           132.0          0.007             0.0                 0.008   \n",
            "\n",
            "   light_decelerations  severe_decelerations  prolongued_decelerations  \\\n",
            "0                0.000                   0.0                       0.0   \n",
            "1                0.003                   0.0                       0.0   \n",
            "2                0.003                   0.0                       0.0   \n",
            "3                0.003                   0.0                       0.0   \n",
            "4                0.000                   0.0                       0.0   \n",
            "\n",
            "   abnormal_short_term_variability  mean_value_of_short_term_variability  \\\n",
            "0                             73.0                                   0.5   \n",
            "1                             17.0                                   2.1   \n",
            "2                             16.0                                   2.1   \n",
            "3                             16.0                                   2.4   \n",
            "4                             16.0                                   2.4   \n",
            "\n",
            "   percentage_of_time_with_abnormal_long_term_variability  ...  histogram_min  \\\n",
            "0                                               43.0       ...           62.0   \n",
            "1                                                0.0       ...           68.0   \n",
            "2                                                0.0       ...           68.0   \n",
            "3                                                0.0       ...           53.0   \n",
            "4                                                0.0       ...           53.0   \n",
            "\n",
            "   histogram_max  histogram_number_of_peaks  histogram_number_of_zeroes  \\\n",
            "0          126.0                        2.0                         0.0   \n",
            "1          198.0                        6.0                         1.0   \n",
            "2          198.0                        5.0                         1.0   \n",
            "3          170.0                       11.0                         0.0   \n",
            "4          170.0                        9.0                         0.0   \n",
            "\n",
            "   histogram_mode  histogram_mean  histogram_median  histogram_variance  \\\n",
            "0           120.0           137.0             121.0                73.0   \n",
            "1           141.0           136.0             140.0                12.0   \n",
            "2           141.0           135.0             138.0                13.0   \n",
            "3           137.0           134.0             137.0                13.0   \n",
            "4           137.0           136.0             138.0                11.0   \n",
            "\n",
            "   histogram_tendency  fetal_health  \n",
            "0                 1.0           2.0  \n",
            "1                 0.0           1.0  \n",
            "2                 0.0           1.0  \n",
            "3                 1.0           1.0  \n",
            "4                 1.0           1.0  \n",
            "\n",
            "[5 rows x 22 columns]\n",
            "\n",
            "Dataset Shape: (2126, 22)\n",
            "\n",
            "Features after dropping less important ones:\n",
            "['accelerations', 'uterine_contractions', 'prolongued_decelerations', 'abnormal_short_term_variability', 'mean_value_of_short_term_variability', 'percentage_of_time_with_abnormal_long_term_variability', 'histogram_mode', 'histogram_mean', 'histogram_median', 'histogram_variance', 'fetal_health']\n",
            "\n",
            "New Dataset Shape after dropping features: (2126, 11)\n",
            "\n",
            "Dataset with Mapped Labels:\n",
            "   fetal_health fetal_health_label\n",
            "0             2            Suspect\n",
            "1             1             Normal\n",
            "2             1             Normal\n",
            "3             1             Normal\n",
            "4             1             Normal\n",
            "\n",
            "Resampled X shape after SMOTE + Tomek Links: (4946, 10)\n",
            "Resampled y distribution after SMOTE + Tomek Links:\n",
            "fetal_health\n",
            "1    1655\n",
            "3    1648\n",
            "2    1643\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Training set shape: (3462, 10)\n",
            "Testing set shape: (1484, 10)\n",
            "\n",
            "Min of Scaled Training Features (Should be 0):\n",
            "accelerations                                             0.0\n",
            "uterine_contractions                                      0.0\n",
            "prolongued_decelerations                                  0.0\n",
            "abnormal_short_term_variability                           0.0\n",
            "mean_value_of_short_term_variability                      0.0\n",
            "percentage_of_time_with_abnormal_long_term_variability    0.0\n",
            "histogram_mode                                            0.0\n",
            "histogram_mean                                            0.0\n",
            "histogram_median                                          0.0\n",
            "histogram_variance                                        0.0\n",
            "dtype: float64\n",
            "\n",
            "Max of Scaled Training Features (Should be 1):\n",
            "accelerations                                             1.0\n",
            "uterine_contractions                                      1.0\n",
            "prolongued_decelerations                                  1.0\n",
            "abnormal_short_term_variability                           1.0\n",
            "mean_value_of_short_term_variability                      1.0\n",
            "percentage_of_time_with_abnormal_long_term_variability    1.0\n",
            "histogram_mode                                            1.0\n",
            "histogram_mean                                            1.0\n",
            "histogram_median                                          1.0\n",
            "histogram_variance                                        1.0\n",
            "dtype: float64\n",
            "\n",
            "Adjusted y_train distribution:\n",
            "fetal_health\n",
            "0    1158\n",
            "2    1154\n",
            "1    1150\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Adjusted y_test distribution:\n",
            "fetal_health\n",
            "0    497\n",
            "2    494\n",
            "1    493\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Final Training set shape: (2769, 10)\n",
            "Validation set shape: (693, 10)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-01-12 07:09:08,148] Trial 0 finished with value: 0.950937950937951 and parameters: {'n_d': 128, 'n_a': 67, 'n_steps': 7, 'gamma': 1.6614778909407883, 'lambda_sparse': 0.0003674811482794505, 'learning_rate': 0.053065035211493534, 'batch_size': 256}. Best is trial 0 with value: 0.950937950937951.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 75 with best_epoch = 55 and best_valid_accuracy = 0.96825\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-01-12 07:12:38,267] Trial 1 finished with value: 0.9682539682539683 and parameters: {'n_d': 69, 'n_a': 76, 'n_steps': 10, 'gamma': 1.036562066833326, 'lambda_sparse': 0.00017310020872191319, 'learning_rate': 0.0025858439594720482, 'batch_size': 128}. Best is trial 1 with value: 0.9682539682539683.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 73 with best_epoch = 53 and best_valid_accuracy = 0.95382\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-01-12 07:16:05,699] Trial 2 finished with value: 0.9538239538239538 and parameters: {'n_d': 121, 'n_a': 65, 'n_steps': 8, 'gamma': 1.5431685896698735, 'lambda_sparse': 0.0010945116691778315, 'learning_rate': 0.011328685093238472, 'batch_size': 128}. Best is trial 1 with value: 0.9682539682539683.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 96 with best_epoch = 76 and best_valid_accuracy = 0.95094\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-01-12 07:18:31,878] Trial 3 finished with value: 0.950937950937951 and parameters: {'n_d': 99, 'n_a': 34, 'n_steps': 8, 'gamma': 1.4585069841194684, 'lambda_sparse': 0.00020678759684493472, 'learning_rate': 0.09935821040527626, 'batch_size': 256}. Best is trial 1 with value: 0.9682539682539683.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stop training because you reached max_epochs = 100 with best_epoch = 99 and best_valid_accuracy = 0.94228\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-01-12 07:21:29,830] Trial 4 finished with value: 0.9422799422799423 and parameters: {'n_d': 101, 'n_a': 95, 'n_steps': 8, 'gamma': 1.5142795723030338, 'lambda_sparse': 0.008783603601289715, 'learning_rate': 0.006670546160823257, 'batch_size': 512}. Best is trial 1 with value: 0.9682539682539683.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 72 with best_epoch = 52 and best_valid_accuracy = 0.95527\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-01-12 07:22:53,845] Trial 5 finished with value: 0.9552669552669553 and parameters: {'n_d': 99, 'n_a': 37, 'n_steps': 6, 'gamma': 1.3641102038496329, 'lambda_sparse': 0.0008296876212410576, 'learning_rate': 0.026007212580657062, 'batch_size': 256}. Best is trial 1 with value: 0.9682539682539683.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stop training because you reached max_epochs = 100 with best_epoch = 80 and best_valid_accuracy = 0.95527\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-01-12 07:24:10,085] Trial 6 finished with value: 0.9552669552669553 and parameters: {'n_d': 51, 'n_a': 64, 'n_steps': 4, 'gamma': 1.8656905177279732, 'lambda_sparse': 0.00010726451072331761, 'learning_rate': 0.05078031831722836, 'batch_size': 256}. Best is trial 1 with value: 0.9682539682539683.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 69 with best_epoch = 49 and best_valid_accuracy = 0.96392\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-01-12 07:25:12,190] Trial 7 finished with value: 0.963924963924964 and parameters: {'n_d': 115, 'n_a': 65, 'n_steps': 3, 'gamma': 1.0429480562724731, 'lambda_sparse': 0.0007551739774701521, 'learning_rate': 0.005218127227572316, 'batch_size': 256}. Best is trial 1 with value: 0.9682539682539683.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stop training because you reached max_epochs = 100 with best_epoch = 90 and best_valid_accuracy = 0.94228\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-01-12 07:26:50,712] Trial 8 finished with value: 0.9422799422799423 and parameters: {'n_d': 46, 'n_a': 112, 'n_steps': 5, 'gamma': 1.7799227601263083, 'lambda_sparse': 0.00834015039026745, 'learning_rate': 0.007121248000041844, 'batch_size': 512}. Best is trial 1 with value: 0.9682539682539683.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 78 with best_epoch = 58 and best_valid_accuracy = 0.96392\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-01-12 07:28:40,494] Trial 9 finished with value: 0.963924963924964 and parameters: {'n_d': 107, 'n_a': 83, 'n_steps': 5, 'gamma': 1.0162394730548785, 'lambda_sparse': 0.0001238417432903007, 'learning_rate': 0.0015244904771003636, 'batch_size': 256}. Best is trial 1 with value: 0.9682539682539683.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 50 with best_epoch = 30 and best_valid_accuracy = 0.94084\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-01-12 07:31:23,338] Trial 10 finished with value: 0.9408369408369408 and parameters: {'n_d': 68, 'n_a': 109, 'n_steps': 10, 'gamma': 1.2532342769982632, 'lambda_sparse': 0.0026388398503359987, 'learning_rate': 0.0011322048274515432, 'batch_size': 128}. Best is trial 1 with value: 0.9682539682539683.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 47 with best_epoch = 27 and best_valid_accuracy = 0.96392\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-01-12 07:32:08,487] Trial 11 finished with value: 0.963924963924964 and parameters: {'n_d': 72, 'n_a': 56, 'n_steps': 3, 'gamma': 1.0444508374914492, 'lambda_sparse': 0.00046123242949527615, 'learning_rate': 0.0028000590058821786, 'batch_size': 128}. Best is trial 1 with value: 0.9682539682539683.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 90 with best_epoch = 70 and best_valid_accuracy = 0.96104\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-01-12 07:36:36,138] Trial 12 finished with value: 0.961038961038961 and parameters: {'n_d': 84, 'n_a': 84, 'n_steps': 10, 'gamma': 1.1857606550935684, 'lambda_sparse': 0.002120385421880758, 'learning_rate': 0.0034361300849609765, 'batch_size': 128}. Best is trial 1 with value: 0.9682539682539683.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 57 with best_epoch = 37 and best_valid_accuracy = 0.97114\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-01-12 07:37:41,904] Trial 13 finished with value: 0.9711399711399712 and parameters: {'n_d': 33, 'n_a': 128, 'n_steps': 3, 'gamma': 1.1761592252229112, 'lambda_sparse': 0.00036511785702883487, 'learning_rate': 0.002956299320724345, 'batch_size': 128}. Best is trial 13 with value: 0.9711399711399712.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stop training because you reached max_epochs = 100 with best_epoch = 83 and best_valid_accuracy = 0.96392\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-01-12 07:41:34,369] Trial 14 finished with value: 0.963924963924964 and parameters: {'n_d': 33, 'n_a': 99, 'n_steps': 9, 'gamma': 1.2322680575282827, 'lambda_sparse': 0.00026983692414714945, 'learning_rate': 0.0021734319948215623, 'batch_size': 128}. Best is trial 13 with value: 0.9711399711399712.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 85 with best_epoch = 65 and best_valid_accuracy = 0.96537\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-01-12 07:44:38,620] Trial 15 finished with value: 0.9653679653679653 and parameters: {'n_d': 56, 'n_a': 125, 'n_steps': 6, 'gamma': 1.1859530680486106, 'lambda_sparse': 0.00019636845417664623, 'learning_rate': 0.013037687984984772, 'batch_size': 128}. Best is trial 13 with value: 0.9711399711399712.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 61 with best_epoch = 41 and best_valid_accuracy = 0.95094\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-01-12 07:45:34,977] Trial 16 finished with value: 0.950937950937951 and parameters: {'n_d': 32, 'n_a': 48, 'n_steps': 4, 'gamma': 1.9900792198671846, 'lambda_sparse': 0.0005026759442447027, 'learning_rate': 0.004030266424807247, 'batch_size': 128}. Best is trial 13 with value: 0.9711399711399712.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 74 with best_epoch = 54 and best_valid_accuracy = 0.96825\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-01-12 07:48:57,201] Trial 17 finished with value: 0.9682539682539683 and parameters: {'n_d': 84, 'n_a': 126, 'n_steps': 7, 'gamma': 1.3740824663704292, 'lambda_sparse': 0.00016282662917101187, 'learning_rate': 0.0018297875834455389, 'batch_size': 128}. Best is trial 13 with value: 0.9711399711399712.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stop training because you reached max_epochs = 100 with best_epoch = 98 and best_valid_accuracy = 0.95671\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-01-12 07:51:22,518] Trial 18 finished with value: 0.9567099567099567 and parameters: {'n_d': 63, 'n_a': 78, 'n_steps': 9, 'gamma': 1.1258275143437144, 'lambda_sparse': 0.0003310441649102684, 'learning_rate': 0.01826128359478549, 'batch_size': 512}. Best is trial 13 with value: 0.9711399711399712.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 72 with best_epoch = 52 and best_valid_accuracy = 0.95382\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-01-12 07:53:09,374] Trial 19 finished with value: 0.9538239538239538 and parameters: {'n_d': 43, 'n_a': 93, 'n_steps': 5, 'gamma': 1.330788454583992, 'lambda_sparse': 0.0013285797077193019, 'learning_rate': 0.0010334815092520459, 'batch_size': 128}. Best is trial 13 with value: 0.9711399711399712.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 82 with best_epoch = 62 and best_valid_accuracy = 0.96681\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-01-12 07:57:28,829] Trial 20 finished with value: 0.9668109668109668 and parameters: {'n_d': 77, 'n_a': 112, 'n_steps': 9, 'gamma': 1.1034189447326803, 'lambda_sparse': 0.0005465362845907321, 'learning_rate': 0.0029228890950667324, 'batch_size': 128}. Best is trial 13 with value: 0.9711399711399712.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 82 with best_epoch = 62 and best_valid_accuracy = 0.9596\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-01-12 08:01:03,321] Trial 21 finished with value: 0.9595959595959596 and parameters: {'n_d': 88, 'n_a': 114, 'n_steps': 7, 'gamma': 1.3414129811597204, 'lambda_sparse': 0.0001640907562665475, 'learning_rate': 0.001866081873148882, 'batch_size': 128}. Best is trial 13 with value: 0.9711399711399712.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 93 with best_epoch = 73 and best_valid_accuracy = 0.96248\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-01-12 08:05:21,911] Trial 22 finished with value: 0.9624819624819625 and parameters: {'n_d': 84, 'n_a': 127, 'n_steps': 7, 'gamma': 1.4214468942829412, 'lambda_sparse': 0.0002334756135464357, 'learning_rate': 0.0014508600133250587, 'batch_size': 128}. Best is trial 13 with value: 0.9711399711399712.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 62 with best_epoch = 42 and best_valid_accuracy = 0.96825\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-01-12 08:06:55,163] Trial 23 finished with value: 0.9682539682539683 and parameters: {'n_d': 60, 'n_a': 120, 'n_steps': 4, 'gamma': 1.28929895439721, 'lambda_sparse': 0.00014294646624166726, 'learning_rate': 0.0022519727233780364, 'batch_size': 128}. Best is trial 13 with value: 0.9711399711399712.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Hyperparameters:  {'n_d': 33, 'n_a': 128, 'n_steps': 3, 'gamma': 1.1761592252229112, 'lambda_sparse': 0.00036511785702883487, 'learning_rate': 0.002956299320724345, 'batch_size': 128}\n",
            "Best Validation Accuracy:  0.9711399711399712\n"
          ]
        }
      ],
      "source": [
        "# Install necessary libraries\n",
        "!pip install pytorch-tabnet\n",
        "!pip install captum\n",
        "!pip install optuna\n",
        "!pip install imbalanced-learn\n",
        "!pip install dask-expr\n",
        "!pip install scikit-learn-contrib\n",
        "!pip install lightgbm\n",
        "\n",
        "# Data manipulation and analysis\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Preprocessing and modeling\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "\n",
        "# Handling imbalanced data\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.under_sampling import TomekLinks\n",
        "\n",
        "# Deep Learning Model\n",
        "from pytorch_tabnet.tab_model import TabNetClassifier\n",
        "\n",
        "# Explainable AI\n",
        "import shap\n",
        "from captum.attr import IntegratedGradients\n",
        "\n",
        "# Hyperparameter Optimization\n",
        "import optuna\n",
        "from optuna import Trial\n",
        "\n",
        "# Suppress warnings for cleaner output\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# For model saving and loading\n",
        "import joblib\n",
        "\n",
        "# Import torch for TabNet\n",
        "import torch\n",
        "\n",
        "# Define the PermutationImportanceTabNet class\n",
        "class PermutationImportanceTabNet(TabNetClassifier):\n",
        "    def __init__(self, input_dim, feature_names, permutation_prob=0.1, importance_decay=0.99, *args, **kwargs):\n",
        "        \"\"\"\n",
        "        Initializes the PermutationImportanceTabNet.\n",
        "\n",
        "        Parameters:\n",
        "        - input_dim (int): Number of input features.\n",
        "        - feature_names (list): List of feature names.\n",
        "        - permutation_prob (float): Probability of applying permutation during a forward pass.\n",
        "        - importance_decay (float): Decay factor for importance scores to smooth over epochs.\n",
        "        - *args, **kwargs: Additional arguments for TabNetClassifier.\n",
        "        \"\"\"\n",
        "        super(PermutationImportanceTabNet, self).__init__(input_dim=input_dim, *args, **kwargs)\n",
        "        self.permutation_prob = permutation_prob\n",
        "        self.importance_scores = torch.zeros(input_dim)\n",
        "        self.importance_decay = importance_decay  # To smooth importance scores\n",
        "        self.feature_names = feature_names  # List of feature names for interpretability\n",
        "\n",
        "    def forward(self, X, y=None):\n",
        "        \"\"\"\n",
        "        Overrides the forward pass to include permutation-based feature importance.\n",
        "\n",
        "        Parameters:\n",
        "        - X (torch.Tensor): Input features.\n",
        "        - y (torch.Tensor, optional): Target labels.\n",
        "\n",
        "        Returns:\n",
        "        - out (torch.Tensor): Model outputs.\n",
        "        - M_loss (float): Mask loss.\n",
        "        \"\"\"\n",
        "        # Original forward pass\n",
        "        out, M_loss = super(PermutationImportanceTabNet, self).forward(X, y)\n",
        "\n",
        "        # Apply permutation with a certain probability\n",
        "        if torch.rand(1).item() < self.permutation_prob:\n",
        "            # Iterate over each feature to assess its importance\n",
        "            for i in range(X.size(1)):\n",
        "                # Clone the input to avoid in-place modifications\n",
        "                X_permuted = X.clone()\n",
        "\n",
        "                # Permute the values of the i-th feature across the batch\n",
        "                X_permuted[:, i] = X_permuted[torch.randperm(X_permuted.size(0)), i]\n",
        "\n",
        "                # Forward pass with permuted feature\n",
        "                out_permuted, _ = super(PermutationImportanceTabNet, self).forward(X_permuted, y)\n",
        "\n",
        "                # Compute predictions\n",
        "                preds = out.argmax(dim=1)\n",
        "                preds_permuted = out_permuted.argmax(dim=1)\n",
        "\n",
        "                # Calculate accuracy\n",
        "                acc = accuracy_score(y.cpu().numpy(), preds.cpu().numpy())\n",
        "                acc_perm = accuracy_score(y.cpu().numpy(), preds_permuted.cpu().numpy())\n",
        "\n",
        "                # Drop in accuracy signifies feature importance\n",
        "                drop = acc - acc_perm\n",
        "\n",
        "                # Update importance scores with decay\n",
        "                self.importance_scores[i] = self.importance_decay * self.importance_scores[i] + (1 - self.importance_decay) * drop\n",
        "\n",
        "            # Normalize importance scores to sum to 1 for interpretability\n",
        "            if self.importance_scores.sum() != 0:\n",
        "                self.importance_scores = self.importance_scores / self.importance_scores.sum()\n",
        "\n",
        "            # Print feature importance scores\n",
        "            print(\"\\nFeature Importance Scores after Permutation:\")\n",
        "            for idx, score in enumerate(self.importance_scores):\n",
        "                feature_name = self.feature_names[idx]\n",
        "                print(f\"{feature_name}: {score.item():.4f}\")\n",
        "\n",
        "        return out, M_loss\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('/content/fetal_health.csv')\n",
        "\n",
        "# Display the first five rows to verify\n",
        "print(\"First five rows of the dataset:\")\n",
        "print(data.head())\n",
        "\n",
        "# Check the shape of the dataset\n",
        "print(f\"\\nDataset Shape: {data.shape}\")\n",
        "\n",
        "# Features to drop based on prior analysis\n",
        "features_to_drop = [\n",
        "    'fetal_movement',\n",
        "    'histogram_width',\n",
        "    'histogram_max',\n",
        "    'mean_value_of_long_term_variability',\n",
        "    'histogram_number_of_peaks',\n",
        "    'light_decelerations',\n",
        "    'histogram_tendency',\n",
        "    'histogram_number_of_zeroes',\n",
        "    'severe_decelerations',\n",
        "    'baseline value',\n",
        "    'histogram_min'\n",
        "]\n",
        "\n",
        "# Drop the specified features\n",
        "data_dropped = data.drop(columns=features_to_drop)\n",
        "\n",
        "# Verify the remaining features\n",
        "print(\"\\nFeatures after dropping less important ones:\")\n",
        "print(data_dropped.columns.tolist())\n",
        "\n",
        "# Check the new shape of the dataset\n",
        "print(f\"\\nNew Dataset Shape after dropping features: {data_dropped.shape}\")\n",
        "\n",
        "# Convert 'fetal_health' to integer\n",
        "data_dropped['fetal_health'] = data_dropped['fetal_health'].astype(int)\n",
        "\n",
        "# Mapping numerical classes to descriptive labels\n",
        "health_mapping = {1: 'Normal', 2: 'Suspect', 3: 'Pathological'}\n",
        "data_dropped['fetal_health_label'] = data_dropped['fetal_health'].map(health_mapping)\n",
        "\n",
        "# Display the mapping\n",
        "print(\"\\nDataset with Mapped Labels:\")\n",
        "print(data_dropped[['fetal_health', 'fetal_health_label']].head())\n",
        "\n",
        "# Features and target\n",
        "X = data_dropped.drop(['fetal_health', 'fetal_health_label'], axis=1)\n",
        "y = data_dropped['fetal_health']\n",
        "\n",
        "# Initialize SMOTE with 'auto' strategy to resample all classes\n",
        "smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
        "\n",
        "# Apply SMOTE to the dataset\n",
        "X_smote, y_smote = smote.fit_resample(X, y)\n",
        "\n",
        "# Initialize Tomek Links\n",
        "tomek = TomekLinks()\n",
        "\n",
        "# Apply Tomek Links to clean the dataset\n",
        "X_resampled, y_resampled = tomek.fit_resample(X_smote, y_smote)\n",
        "\n",
        "# Display the shape of the resampled dataset and class distribution\n",
        "print(f\"\\nResampled X shape after SMOTE + Tomek Links: {X_resampled.shape}\")\n",
        "print(f\"Resampled y distribution after SMOTE + Tomek Links:\\n{y_resampled.value_counts()}\")\n",
        "\n",
        "# Split the resampled data (70% train, 30% test) with stratification\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_resampled, y_resampled, test_size=0.3, random_state=42, stratify=y_resampled\n",
        ")\n",
        "\n",
        "# Display the shapes of the training and testing sets\n",
        "print(f\"\\nTraining set shape: {X_train.shape}\")\n",
        "print(f\"Testing set shape: {X_test.shape}\")\n",
        "\n",
        "# Initialize the MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# Fit the scaler on the training data and transform\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "\n",
        "# Transform the testing data\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Convert the scaled arrays back to DataFrames for easier handling\n",
        "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X.columns, index=X_train.index)\n",
        "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X.columns, index=X_test.index)\n",
        "\n",
        "# Verify scaling by checking min and max values\n",
        "print(\"\\nMin of Scaled Training Features (Should be 0):\")\n",
        "print(X_train_scaled.min())\n",
        "\n",
        "print(\"\\nMax of Scaled Training Features (Should be 1):\")\n",
        "print(X_train_scaled.max())\n",
        "\n",
        "# Adjust the target values so they start from 0\n",
        "y_train = y_train - 1\n",
        "y_test = y_test - 1\n",
        "\n",
        "# Display the adjusted target distributions\n",
        "print(\"\\nAdjusted y_train distribution:\")\n",
        "print(pd.Series(y_train).value_counts())\n",
        "\n",
        "print(\"\\nAdjusted y_test distribution:\")\n",
        "print(pd.Series(y_test).value_counts())\n",
        "\n",
        "# Further split the training data into training and validation sets\n",
        "X_train_final, X_valid, y_train_final, y_valid = train_test_split(\n",
        "    X_train_scaled, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
        ")\n",
        "\n",
        "# Display the shapes of the final training and validation sets\n",
        "print(f\"\\nFinal Training set shape: {X_train_final.shape}\")\n",
        "print(f\"Validation set shape: {X_valid.shape}\")\n",
        "\n",
        "# -------------------\n",
        "# Hyperparameter Optimization with Optuna\n",
        "# -------------------\n",
        "def objective(trial: Trial):\n",
        "    # Define the hyperparameter space\n",
        "    n_d = trial.suggest_int('n_d', 32, 128)\n",
        "    n_a = trial.suggest_int('n_a', 32, 128)\n",
        "    n_steps = trial.suggest_int('n_steps', 3, 10)\n",
        "    gamma = trial.suggest_float('gamma', 1.0, 2.0)\n",
        "    lambda_sparse = trial.suggest_float('lambda_sparse', 1e-4, 1e-2, log=True)\n",
        "    learning_rate = trial.suggest_float('learning_rate', 1e-3, 1e-1, log=True)\n",
        "    batch_size = trial.suggest_categorical('batch_size', [128, 256, 512])\n",
        "\n",
        "    # Initialize TabNet with current hyperparameters\n",
        "    tabnet = TabNetClassifier(\n",
        "        n_d=n_d,\n",
        "        n_a=n_a,\n",
        "        n_steps=n_steps,\n",
        "        gamma=gamma,\n",
        "        lambda_sparse=lambda_sparse,\n",
        "        optimizer_fn=torch.optim.Adam,\n",
        "        optimizer_params=dict(lr=learning_rate),\n",
        "        mask_type='sparsemax',\n",
        "        verbose=0\n",
        "    )\n",
        "\n",
        "    # Train the model on the final training set\n",
        "    tabnet.fit(\n",
        "        X_train=X_train_final.values,\n",
        "        y_train=y_train_final.values,\n",
        "        eval_set=[(X_valid.values, y_valid.values)],\n",
        "        eval_name=['valid'],\n",
        "        eval_metric=['accuracy'],\n",
        "        max_epochs=100,\n",
        "        patience=20,\n",
        "        batch_size=batch_size,\n",
        "        virtual_batch_size=128\n",
        "    )\n",
        "\n",
        "    # Predict on the validation set\n",
        "    y_pred = tabnet.predict(X_valid.values)\n",
        "    accuracy = accuracy_score(y_valid, y_pred)\n",
        "\n",
        "    return accuracy\n",
        "\n",
        "# Create and optimize the Optuna study\n",
        "study = optuna.create_study(direction='maximize')\n",
        "study.optimize(objective, n_trials=50, timeout=3600)  # Adjust n_trials and timeout as needed\n",
        "\n",
        "print(\"Best Hyperparameters: \", study.best_params)\n",
        "print(\"Best Validation Accuracy: \", study.best_value)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. Retrain TabNet with Best Hyperparameters Using PermutationImportanceTabNet\n",
        "# -------------------\n",
        "# Extract best hyperparameters\n",
        "best_params = study.best_params\n",
        "\n",
        "# Adjust keys if necessary\n",
        "# (Ensure that 'learning_rate' and 'batch_size' are correctly handled)\n",
        "# In this case, no adjustment is needed as keys are consistent\n",
        "\n",
        "# Define feature names for interpretability\n",
        "feature_names = X.columns.tolist()\n",
        "\n",
        "# Determine the input dimension from the training data\n",
        "input_dim = X_train_final.shape[1]\n",
        "\n",
        "# Initialize the Permutation Importance TabNet with the correct input_dim and feature_names\n",
        "perm_importance_tabnet = PermutationImportanceTabNet(\n",
        "    input_dim=input_dim,\n",
        "    feature_names=feature_names,\n",
        "    n_d=best_params['n_d'],\n",
        "    n_a=best_params['n_a'],\n",
        "    n_steps=best_params['n_steps'],\n",
        "    gamma=best_params['gamma'],\n",
        "    lambda_sparse=best_params['lambda_sparse'],\n",
        "    optimizer_fn=torch.optim.Adam,\n",
        "    optimizer_params=dict(lr=best_params['learning_rate']),\n",
        "    mask_type='sparsemax',\n",
        "    permutation_prob=0.1,          # 10% chance to apply permutation\n",
        "    importance_decay=0.99,         # Decay factor for smoothing\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Train the Permutation Importance TabNet model\n",
        "perm_importance_tabnet.fit(\n",
        "    X_train=X_train_final.values,\n",
        "    y_train=y_train_final.values,\n",
        "    eval_set=[(X_valid.values, y_valid.values), (X_test_scaled.values, y_test.values)],\n",
        "    eval_name=['train', 'valid'],\n",
        "    eval_metric=['accuracy'],\n",
        "    max_epochs=100,\n",
        "    patience=20,\n",
        "    batch_size=best_params['batch_size'],\n",
        "    virtual_batch_size=128\n",
        ")\n",
        "\n",
        "# Predict and evaluate on the test set\n",
        "y_pred_perm_importance = perm_importance_tabnet.predict(X_test_scaled.values)\n",
        "print(\"\\nPermutation Importance TabNet Classification Report:\")\n",
        "print(classification_report(y_test, y_pred_perm_importance, target_names=['Normal', 'Suspect', 'Pathological']))\n",
        "\n",
        "# Access and print feature importance scores\n",
        "print(\"\\nFeature Importance Scores:\")\n",
        "for idx, score in enumerate(perm_importance_tabnet.importance_scores):\n",
        "    feature_name = perm_importance_tabnet.feature_names[idx]\n",
        "    print(f\"{feature_name}: {score.item():.4f}\")"
      ],
      "metadata": {
        "id": "aQxqqqZCKcC8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d75c04e-8735-49ba-b857-ad131d9ac7a5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0  | loss: 0.73588 | train_accuracy: 0.34632 | valid_accuracy: 0.35243 |  0:00:03s\n",
            "epoch 1  | loss: 0.36865 | train_accuracy: 0.37807 | valid_accuracy: 0.36927 |  0:00:08s\n",
            "epoch 2  | loss: 0.29494 | train_accuracy: 0.5267  | valid_accuracy: 0.51011 |  0:00:15s\n",
            "epoch 3  | loss: 0.26794 | train_accuracy: 0.57576 | valid_accuracy: 0.58288 |  0:00:20s\n",
            "epoch 4  | loss: 0.24623 | train_accuracy: 0.56999 | valid_accuracy: 0.56469 |  0:00:24s\n",
            "epoch 5  | loss: 0.24563 | train_accuracy: 0.63203 | valid_accuracy: 0.65499 |  0:00:29s\n",
            "epoch 6  | loss: 0.23157 | train_accuracy: 0.65945 | valid_accuracy: 0.66509 |  0:00:32s\n",
            "epoch 7  | loss: 0.20214 | train_accuracy: 0.68398 | valid_accuracy: 0.69879 |  0:00:33s\n",
            "epoch 8  | loss: 0.21204 | train_accuracy: 0.7215  | valid_accuracy: 0.73248 |  0:00:35s\n",
            "epoch 9  | loss: 0.20335 | train_accuracy: 0.73737 | valid_accuracy: 0.7473  |  0:00:37s\n",
            "epoch 10 | loss: 0.19136 | train_accuracy: 0.79654 | valid_accuracy: 0.78976 |  0:00:40s\n",
            "epoch 11 | loss: 0.16558 | train_accuracy: 0.77633 | valid_accuracy: 0.79717 |  0:00:45s\n",
            "epoch 12 | loss: 0.17982 | train_accuracy: 0.81385 | valid_accuracy: 0.80795 |  0:00:46s\n",
            "epoch 13 | loss: 0.20707 | train_accuracy: 0.83838 | valid_accuracy: 0.8403  |  0:00:48s\n",
            "epoch 14 | loss: 0.17159 | train_accuracy: 0.88312 | valid_accuracy: 0.86253 |  0:00:49s\n",
            "epoch 15 | loss: 0.17293 | train_accuracy: 0.90188 | valid_accuracy: 0.90162 |  0:00:50s\n",
            "epoch 16 | loss: 0.17522 | train_accuracy: 0.87879 | valid_accuracy: 0.87736 |  0:00:52s\n",
            "epoch 17 | loss: 0.15435 | train_accuracy: 0.90188 | valid_accuracy: 0.89488 |  0:00:54s\n",
            "epoch 18 | loss: 0.18049 | train_accuracy: 0.93218 | valid_accuracy: 0.93396 |  0:00:58s\n",
            "epoch 19 | loss: 0.15427 | train_accuracy: 0.94949 | valid_accuracy: 0.94744 |  0:01:01s\n",
            "epoch 20 | loss: 0.15117 | train_accuracy: 0.91775 | valid_accuracy: 0.93059 |  0:01:02s\n",
            "epoch 21 | loss: 0.15106 | train_accuracy: 0.93074 | valid_accuracy: 0.9535  |  0:01:03s\n",
            "epoch 22 | loss: 0.16507 | train_accuracy: 0.94228 | valid_accuracy: 0.93868 |  0:01:04s\n",
            "epoch 23 | loss: 0.16472 | train_accuracy: 0.94372 | valid_accuracy: 0.9535  |  0:01:06s\n",
            "epoch 24 | loss: 0.16142 | train_accuracy: 0.95238 | valid_accuracy: 0.95553 |  0:01:07s\n",
            "epoch 25 | loss: 0.14535 | train_accuracy: 0.94805 | valid_accuracy: 0.94744 |  0:01:08s\n",
            "epoch 26 | loss: 0.15302 | train_accuracy: 0.94805 | valid_accuracy: 0.95148 |  0:01:09s\n",
            "epoch 27 | loss: 0.16808 | train_accuracy: 0.94805 | valid_accuracy: 0.9562  |  0:01:11s\n",
            "epoch 28 | loss: 0.14574 | train_accuracy: 0.95094 | valid_accuracy: 0.95081 |  0:01:13s\n",
            "epoch 29 | loss: 0.15077 | train_accuracy: 0.95527 | valid_accuracy: 0.95957 |  0:01:14s\n",
            "epoch 30 | loss: 0.14472 | train_accuracy: 0.95238 | valid_accuracy: 0.95553 |  0:01:15s\n",
            "epoch 31 | loss: 0.13153 | train_accuracy: 0.94517 | valid_accuracy: 0.95822 |  0:01:17s\n",
            "epoch 32 | loss: 0.12849 | train_accuracy: 0.96248 | valid_accuracy: 0.96159 |  0:01:18s\n",
            "epoch 33 | loss: 0.12532 | train_accuracy: 0.96392 | valid_accuracy: 0.95755 |  0:01:19s\n",
            "epoch 34 | loss: 0.13319 | train_accuracy: 0.94949 | valid_accuracy: 0.95822 |  0:01:20s\n",
            "epoch 35 | loss: 0.1204  | train_accuracy: 0.95238 | valid_accuracy: 0.95553 |  0:01:22s\n",
            "epoch 36 | loss: 0.14255 | train_accuracy: 0.96392 | valid_accuracy: 0.96698 |  0:01:23s\n",
            "epoch 37 | loss: 0.12223 | train_accuracy: 0.9596  | valid_accuracy: 0.96698 |  0:01:24s\n",
            "epoch 38 | loss: 0.10668 | train_accuracy: 0.96537 | valid_accuracy: 0.97035 |  0:01:26s\n",
            "epoch 39 | loss: 0.11583 | train_accuracy: 0.95671 | valid_accuracy: 0.96631 |  0:01:28s\n",
            "epoch 40 | loss: 0.1133  | train_accuracy: 0.96537 | valid_accuracy: 0.96159 |  0:01:29s\n",
            "epoch 41 | loss: 0.1109  | train_accuracy: 0.95382 | valid_accuracy: 0.95687 |  0:01:30s\n",
            "epoch 42 | loss: 0.13135 | train_accuracy: 0.95382 | valid_accuracy: 0.95822 |  0:01:31s\n",
            "epoch 43 | loss: 0.12441 | train_accuracy: 0.95671 | valid_accuracy: 0.95889 |  0:01:32s\n",
            "epoch 44 | loss: 0.10274 | train_accuracy: 0.9697  | valid_accuracy: 0.96226 |  0:01:34s\n",
            "epoch 45 | loss: 0.1087  | train_accuracy: 0.96681 | valid_accuracy: 0.96765 |  0:01:35s\n",
            "epoch 46 | loss: 0.12411 | train_accuracy: 0.95094 | valid_accuracy: 0.96024 |  0:01:36s\n",
            "epoch 47 | loss: 0.11344 | train_accuracy: 0.96248 | valid_accuracy: 0.9562  |  0:01:38s\n",
            "epoch 48 | loss: 0.13267 | train_accuracy: 0.96248 | valid_accuracy: 0.94542 |  0:01:40s\n",
            "epoch 49 | loss: 0.1061  | train_accuracy: 0.96392 | valid_accuracy: 0.96092 |  0:01:42s\n",
            "epoch 50 | loss: 0.12756 | train_accuracy: 0.96248 | valid_accuracy: 0.96429 |  0:01:43s\n",
            "epoch 51 | loss: 0.12011 | train_accuracy: 0.97114 | valid_accuracy: 0.96968 |  0:01:44s\n",
            "epoch 52 | loss: 0.11831 | train_accuracy: 0.96392 | valid_accuracy: 0.95081 |  0:01:45s\n",
            "epoch 53 | loss: 0.10942 | train_accuracy: 0.96537 | valid_accuracy: 0.96429 |  0:01:47s\n",
            "epoch 54 | loss: 0.10745 | train_accuracy: 0.96104 | valid_accuracy: 0.96631 |  0:01:48s\n",
            "epoch 55 | loss: 0.11095 | train_accuracy: 0.96392 | valid_accuracy: 0.96024 |  0:01:49s\n",
            "epoch 56 | loss: 0.11008 | train_accuracy: 0.93362 | valid_accuracy: 0.9407  |  0:01:50s\n",
            "epoch 57 | loss: 0.10257 | train_accuracy: 0.94372 | valid_accuracy: 0.95418 |  0:01:52s\n",
            "epoch 58 | loss: 0.11634 | train_accuracy: 0.96537 | valid_accuracy: 0.97102 |  0:01:54s\n",
            "epoch 59 | loss: 0.10004 | train_accuracy: 0.95815 | valid_accuracy: 0.95485 |  0:01:55s\n",
            "epoch 60 | loss: 0.11139 | train_accuracy: 0.96104 | valid_accuracy: 0.96024 |  0:01:57s\n",
            "epoch 61 | loss: 0.10825 | train_accuracy: 0.96537 | valid_accuracy: 0.96968 |  0:01:58s\n",
            "epoch 62 | loss: 0.10139 | train_accuracy: 0.96681 | valid_accuracy: 0.96294 |  0:01:59s\n",
            "epoch 63 | loss: 0.10059 | train_accuracy: 0.97691 | valid_accuracy: 0.96631 |  0:02:00s\n",
            "epoch 64 | loss: 0.09849 | train_accuracy: 0.96248 | valid_accuracy: 0.97102 |  0:02:02s\n",
            "epoch 65 | loss: 0.1223  | train_accuracy: 0.9697  | valid_accuracy: 0.969   |  0:02:03s\n",
            "epoch 66 | loss: 0.09779 | train_accuracy: 0.96392 | valid_accuracy: 0.96833 |  0:02:04s\n",
            "epoch 67 | loss: 0.10192 | train_accuracy: 0.96681 | valid_accuracy: 0.97237 |  0:02:05s\n",
            "epoch 68 | loss: 0.10148 | train_accuracy: 0.96248 | valid_accuracy: 0.9717  |  0:02:07s\n",
            "epoch 69 | loss: 0.12908 | train_accuracy: 0.95671 | valid_accuracy: 0.95013 |  0:02:09s\n",
            "epoch 70 | loss: 0.09535 | train_accuracy: 0.96681 | valid_accuracy: 0.96765 |  0:02:10s\n",
            "epoch 71 | loss: 0.11815 | train_accuracy: 0.96104 | valid_accuracy: 0.97237 |  0:02:11s\n",
            "epoch 72 | loss: 0.09398 | train_accuracy: 0.96392 | valid_accuracy: 0.96563 |  0:02:13s\n",
            "epoch 73 | loss: 0.09136 | train_accuracy: 0.97114 | valid_accuracy: 0.96765 |  0:02:14s\n",
            "epoch 74 | loss: 0.09323 | train_accuracy: 0.9697  | valid_accuracy: 0.96765 |  0:02:15s\n",
            "epoch 75 | loss: 0.10315 | train_accuracy: 0.96825 | valid_accuracy: 0.96429 |  0:02:16s\n",
            "epoch 76 | loss: 0.1017  | train_accuracy: 0.95815 | valid_accuracy: 0.96698 |  0:02:17s\n",
            "epoch 77 | loss: 0.10476 | train_accuracy: 0.97114 | valid_accuracy: 0.97035 |  0:02:19s\n",
            "epoch 78 | loss: 0.08899 | train_accuracy: 0.96392 | valid_accuracy: 0.96833 |  0:02:20s\n",
            "epoch 79 | loss: 0.09285 | train_accuracy: 0.95815 | valid_accuracy: 0.96563 |  0:02:22s\n",
            "epoch 80 | loss: 0.08702 | train_accuracy: 0.96104 | valid_accuracy: 0.969   |  0:02:23s\n",
            "epoch 81 | loss: 0.08595 | train_accuracy: 0.96104 | valid_accuracy: 0.96024 |  0:02:25s\n",
            "epoch 82 | loss: 0.08721 | train_accuracy: 0.96392 | valid_accuracy: 0.95957 |  0:02:26s\n",
            "epoch 83 | loss: 0.09449 | train_accuracy: 0.97403 | valid_accuracy: 0.96698 |  0:02:27s\n",
            "epoch 84 | loss: 0.09766 | train_accuracy: 0.96392 | valid_accuracy: 0.96765 |  0:02:28s\n",
            "epoch 85 | loss: 0.08141 | train_accuracy: 0.97258 | valid_accuracy: 0.9717  |  0:02:29s\n",
            "epoch 86 | loss: 0.10081 | train_accuracy: 0.9697  | valid_accuracy: 0.96294 |  0:02:31s\n",
            "epoch 87 | loss: 0.11092 | train_accuracy: 0.95671 | valid_accuracy: 0.96429 |  0:02:32s\n",
            "\n",
            "Early stopping occurred at epoch 87 with best_epoch = 67 and best_valid_accuracy = 0.97237\n",
            "\n",
            "Permutation Importance TabNet Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      Normal       0.99      0.95      0.97       497\n",
            "     Suspect       0.96      0.97      0.96       493\n",
            "Pathological       0.97      1.00      0.99       494\n",
            "\n",
            "    accuracy                           0.97      1484\n",
            "   macro avg       0.97      0.97      0.97      1484\n",
            "weighted avg       0.97      0.97      0.97      1484\n",
            "\n",
            "\n",
            "Feature Importance Scores:\n",
            "accelerations: 0.0000\n",
            "uterine_contractions: 0.0000\n",
            "prolongued_decelerations: 0.0000\n",
            "abnormal_short_term_variability: 0.0000\n",
            "mean_value_of_short_term_variability: 0.0000\n",
            "percentage_of_time_with_abnormal_long_term_variability: 0.0000\n",
            "histogram_mode: 0.0000\n",
            "histogram_mean: 0.0000\n",
            "histogram_median: 0.0000\n",
            "histogram_variance: 0.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KKDkT4lrJvil"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VL5byDq7Jvna"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oYJYf7JwJvpo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-m0An-ZWJvsH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "47O-fye1JvuG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pbsNDkmbJvwc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}